{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fwkxPaZaMFv"
   },
   "source": [
    "# **Deep Reinforcement Learning Class Spring 2025 Assignment 1**\n",
    "\n",
    "In this assignment, we will learn about gym interface, gridworld, q-learning, and etc. You will need to fill in the missing code snippets (marked by TODO).\n",
    "\n",
    "Make a copy of this notebook using File > Save a copy in Drive and edit it with your answers.\n",
    "\n",
    "WARNING: Do not put your name or any other personal identification information in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gym) (2.1.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\yuhun\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833987E30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833E45F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833E46300>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833E46510>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833E466F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "ERROR: Could not find a version that satisfies the requirement numpy==1.24.0 (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\yuhun\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for numpy==1.24.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\yuhun\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install numpy==1.24.0\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L8nLFy4z2bZh"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import importlib.util\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W_kyHQESMG0E"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "suK5e6_R2Fcq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This environment allows you to verify whether your program runs correctly during testing,\n",
    "# as it follows the same observation format from `env.reset()` and `env.step()`.\n",
    "# However, keep in mind that this is just a simplified environment.\n",
    "# The full specifications for the real testing environment can be found in the provided spec.\n",
    "#\n",
    "# You are free to modify this file to better match the real environment and train your own agent.\n",
    "# Good luck!\n",
    "\n",
    "\n",
    "class SimpleTaxiEnv(gym.Env):\n",
    "    def __init__(self, grid_size=10, fuel_limit=5000):\n",
    "        \"\"\"\n",
    "        Custom Taxi environment supporting different grid sizes.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.fuel_limit = fuel_limit\n",
    "        self.current_fuel = fuel_limit\n",
    "        self.passenger_picked_up = False\n",
    "\n",
    "        self.stations = [(0, 0), (0, self.grid_size - 1), (self.grid_size - 1, 0), (self.grid_size - 1, self.grid_size - 1)]\n",
    "        self.passenger_loc = None\n",
    "\n",
    "        self.obstacles = set()  # No obstacles in simple version\n",
    "        self.destination = None\n",
    "    def legal(self):\n",
    "      available_positions = [\n",
    "            (x, y) for x in range(self.grid_size) for y in range(self.grid_size)\n",
    "            if (x, y) not in self.obstacles\n",
    "      ]\n",
    "      n=len(available_positions)\n",
    "      p = [0 for i in range(n)]\n",
    "      global group_size\n",
    "      group_size = n\n",
    "      def find_parent(x):\n",
    "        if x!=p[x]:\n",
    "          p[x]=find_parent(p[x])\n",
    "        return p[x]\n",
    "      def Union(a,b):\n",
    "        global group_size\n",
    "        a=find_parent(a)\n",
    "        b=find_parent(b)\n",
    "        if a!=b:\n",
    "          group_size -= 1\n",
    "          if np.random.randint(2)==0:\n",
    "            p[a]=b\n",
    "          else:\n",
    "            p[b]=a\n",
    "      for i in range(n):\n",
    "        p[i]=i\n",
    "      for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "          if abs(available_positions[i][0]-available_positions[j][0])+abs(available_positions[i][1]-available_positions[j][1])==1:\n",
    "            Union(i,j)\n",
    "      return group_size==1\n",
    "    def reset(self,grid_size=None,obstacle_len=None):\n",
    "        \"\"\"Reset the environment, ensuring Taxi, passenger, and destination are not overlapping obstacles\"\"\"\n",
    "        self.current_fuel = self.fuel_limit\n",
    "        self.passenger_picked_up = False\n",
    "        if grid_size == None:\n",
    "            grid_size = np.random.randint(5,11)\n",
    "        self.grid_size = grid_size\n",
    "        self.stations = []\n",
    "        if obstacle_len == None:\n",
    "            obstacle_len = np.random.randint(0,grid_size*grid_size)\n",
    "        while len(self.stations)<4:\n",
    "            posx,posy = np.random.randint(0,self.grid_size),np.random.randint(0,self.grid_size)\n",
    "            # check posx,posy is not adjacent to existed stations\n",
    "            if not any([abs(posx-station[0])+abs(posy-station[1])<=1 for station in self.stations]):\n",
    "                self.stations.append((posx,posy))\n",
    "        #self.stations = [(0, 0), (0, self.grid_size - 1), (self.grid_size - 1, 0), (self.grid_size - 1, self.grid_size - 1)]\n",
    "        self.obstacles = set()\n",
    "        cnt = 0\n",
    "        while len(self.obstacles)<obstacle_len and cnt < self.grid_size*self.grid_size*10:\n",
    "            posx,posy = np.random.randint(0,self.grid_size),np.random.randint(0,self.grid_size)\n",
    "            if (posx,posy) not in self.stations and (posx,posy) not in self.obstacles:\n",
    "                self.obstacles.add((posx,posy))\n",
    "            if not self.legal():\n",
    "                self.obstacles.remove((posx,posy))\n",
    "            cnt = cnt +1\n",
    "        available_positions = [\n",
    "            (x, y) for x in range(self.grid_size) for y in range(self.grid_size)\n",
    "            if (x, y) not in self.stations and (x, y) not in self.obstacles\n",
    "        ]\n",
    "\n",
    "        self.taxi_pos = random.choice(available_positions)\n",
    "\n",
    "        self.passenger_loc = random.choice([pos for pos in self.stations])\n",
    "\n",
    "\n",
    "        possible_destinations = [s for s in self.stations if s != self.passenger_loc]\n",
    "        self.destination = random.choice(possible_destinations)\n",
    "        #print(f'grid size : {self.grid_size}, obstacles : {len(self.obstacles)}, ')\n",
    "        #self.render_env(self.taxi_pos)\n",
    "        return self.get_state(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Perform an action and update the environment state.\"\"\"\n",
    "        taxi_row, taxi_col = self.taxi_pos\n",
    "        next_row, next_col = taxi_row, taxi_col\n",
    "        reward = 0\n",
    "        if action == 0 :  # Move Down\n",
    "            next_row += 1\n",
    "        elif action == 1:  # Move Up\n",
    "            next_row -= 1\n",
    "        elif action == 2:  # Move Right\n",
    "            next_col += 1\n",
    "        elif action == 3:  # Move Left\n",
    "            next_col -= 1\n",
    "\n",
    "\n",
    "        if action in [0, 1, 2, 3]:  # Only movement actions should be checked\n",
    "            if (next_row, next_col) in self.obstacles or not (0 <= next_row < self.grid_size and 0 <= next_col < self.grid_size):\n",
    "                reward -=10\n",
    "                #print('block')\n",
    "            else:\n",
    "                self.taxi_pos = (next_row, next_col)\n",
    "                if self.passenger_picked_up:\n",
    "                    self.passenger_loc = self.taxi_pos\n",
    "        else:\n",
    "            if action == 4:  # PICKUP\n",
    "                if self.taxi_pos == self.passenger_loc:\n",
    "                    self.passenger_picked_up = True\n",
    "                    self.passenger_loc = self.taxi_pos\n",
    "                else:\n",
    "                    reward = -10\n",
    "            elif action == 5:  # DROPOFF\n",
    "                if self.passenger_picked_up:\n",
    "                    self.passenger_picked_up = False\n",
    "                    self.passenger_loc = self.taxi_pos\n",
    "                    if self.taxi_pos == self.destination:\n",
    "                        reward += 50\n",
    "                        return self.get_state(), reward -0.1, True, {}\n",
    "                    else:\n",
    "                        reward -=10\n",
    "                else:\n",
    "                    reward -=10\n",
    "\n",
    "        reward -= 0.1\n",
    "\n",
    "        self.current_fuel -= 1\n",
    "        if self.current_fuel <= 0:\n",
    "            return self.get_state(), reward -10, True, {}\n",
    "\n",
    "\n",
    "\n",
    "        return self.get_state(), reward, False, {}\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Return the current environment state.\"\"\"\n",
    "        taxi_row, taxi_col = self.taxi_pos\n",
    "        passenger_row, passenger_col = self.passenger_loc\n",
    "        destination_row, destination_col = self.destination\n",
    "\n",
    "        obstacle_north = int(taxi_row == 0 or (taxi_row-1, taxi_col) in self.obstacles)\n",
    "        obstacle_south = int(taxi_row == self.grid_size - 1 or (taxi_row+1, taxi_col) in self.obstacles)\n",
    "        obstacle_east  = int(taxi_col == self.grid_size - 1 or (taxi_row, taxi_col+1) in self.obstacles)\n",
    "        obstacle_west  = int(taxi_col == 0 or (taxi_row , taxi_col-1) in self.obstacles)\n",
    "\n",
    "        passenger_loc_north = int((taxi_row - 1, taxi_col) == self.passenger_loc)\n",
    "        passenger_loc_south = int((taxi_row + 1, taxi_col) == self.passenger_loc)\n",
    "        passenger_loc_east  = int((taxi_row, taxi_col + 1) == self.passenger_loc)\n",
    "        passenger_loc_west  = int((taxi_row, taxi_col - 1) == self.passenger_loc)\n",
    "        passenger_loc_middle  = int( (taxi_row, taxi_col) == self.passenger_loc)\n",
    "        passenger_look = passenger_loc_north or passenger_loc_south or passenger_loc_east or passenger_loc_west or passenger_loc_middle\n",
    "\n",
    "        destination_loc_north = int( (taxi_row - 1, taxi_col) == self.destination)\n",
    "        destination_loc_south = int( (taxi_row + 1, taxi_col) == self.destination)\n",
    "        destination_loc_east  = int( (taxi_row, taxi_col + 1) == self.destination)\n",
    "        destination_loc_west  = int( (taxi_row, taxi_col - 1) == self.destination)\n",
    "        destination_loc_middle  = int( (taxi_row, taxi_col) == self.destination)\n",
    "        destination_look = destination_loc_north or destination_loc_south or destination_loc_east or destination_loc_west or destination_loc_middle\n",
    "\n",
    "\n",
    "        state = (taxi_row, taxi_col, self.stations[0][0],self.stations[0][1] ,self.stations[1][0],self.stations[1][1],self.stations[2][0],self.stations[2][1],self.stations[3][0],self.stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look)\n",
    "        return state,self.passenger_picked_up,self.passenger_loc\n",
    "    def render_env(self, taxi_pos,   action=None, step=None, fuel=None):\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        grid = [['.'] * self.grid_size for _ in range(self.grid_size)]\n",
    "\n",
    "        '''\n",
    "        # Place passenger\n",
    "        py, px = passenger_pos\n",
    "        if 0 <= px < self.grid_size and 0 <= py < self.grid_size:\n",
    "            grid[py][px] = 'P'\n",
    "        '''\n",
    "        stations_char =['R','G','B','Y']\n",
    "        for i in range(len(self.stations)):\n",
    "            sx,sy = self.stations[i]\n",
    "            grid[sy][sx] = stations_char[i]\n",
    "        for ox,oy in self.obstacles:\n",
    "            grid[oy][ox] = '#'\n",
    "\n",
    "        '''\n",
    "        # Place destination\n",
    "        dy, dx = destination_pos\n",
    "        if 0 <= dx < self.grid_size and 0 <= dy < self.grid_size:\n",
    "            grid[dy][dx] = 'D'\n",
    "        '''\n",
    "        # Place taxi\n",
    "        ty, tx = taxi_pos\n",
    "        if 0 <= tx < self.grid_size and 0 <= ty < self.grid_size:\n",
    "            grid[ty][tx] = '🚖'\n",
    "\n",
    "        # Print step info\n",
    "        print(f\"\\nStep: {step}\")\n",
    "        print(f\"Taxi Position: ({tx}, {ty})\")\n",
    "        #print(f\"Passenger Position: ({px}, {py}) {'(In Taxi)' if (px, py) == (tx, ty) else ''}\")\n",
    "        #print(f\"Destination: ({dx}, {dy})\")\n",
    "        print(f\"Fuel Left: {fuel}\")\n",
    "        print(f\"Last Action: {self.get_action_name(action)}\\n\")\n",
    "\n",
    "        # Print grid\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def get_action_name(self, action):\n",
    "        \"\"\"Returns a human-readable action name.\"\"\"\n",
    "        actions = [\"Move South\", \"Move North\", \"Move East\", \"Move West\", \"Pick Up\", \"Drop Off\"]\n",
    "        return actions[action] if action is not None else \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "henzhTux2qUO"
   },
   "source": [
    "## 狀態定義\n",
    "\n",
    "## 狀態定義\n",
    "\n",
    "- pos (相對 R/G/Y/B)的距離 (用 +-0)去代表，直到走到一位置確認是否 pickup/destination為止後 random 換位置。\n",
    "    - 只要相對位移>0一律用 +1表示, <0 用 -1\n",
    "- obstacles\n",
    "- passenger state\n",
    "    - 0 ⇒ not on car\n",
    "    - 1 ⇒ on car\n",
    "- candidates_p\n",
    "    - original : stations\n",
    "    - whenever one is not worked ⇒ removed it\n",
    "- mask_p= [1] * len(candidates_p) + [0] * (4-len(candidates_p))\n",
    "    - 0 代表 passenger_i 是不可能的位置\n",
    "    - 1 代表可能\n",
    "- candidates_goal\n",
    "    - original : stations\n",
    "    - whenever one is not goal ⇒ removed it\n",
    "- mask_goal= [1] * len(candidates_goal) + [0] * (4-len(candidates_goal))\n",
    "    - 0 代表 passenger_i 是不可能的位置\n",
    "    - 1 代表可能\n",
    "- pickup\n",
    "- goal_id : {0,1,2,3}\n",
    "    - 目標是 stations[goal_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKN9Ot4BpJZM"
   },
   "source": [
    "### 轉移\n",
    "\n",
    "- relative_pos = (0,0)\n",
    "    - if passenger_look\n",
    "        - candidate_p = [pos]\n",
    "        - success pickup\n",
    "            - success pickup ⇒ pickup_id=True\n",
    "            - candidates_p = [pos]\n",
    "            - goal = random choice (candidates_goal)\n",
    "    - else:\n",
    "        - candidate_p remove pos\n",
    "        - goal = random.choice(candidates_p)\n",
    "    - if goal_look\n",
    "        - candidate_g = [pos]\n",
    "    - else:\n",
    "        - candidate_g.remove(pos)\n",
    "        - goal = random choice (candidates_goal)\n",
    "- drop\n",
    "    - if pickup ⇒ candidates_p = [pos], goal_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "vDSsEzx-2ndf"
   },
   "outputs": [],
   "source": [
    "from re import M\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "def tabular_q_learning(env,episodes=5000, alpha=0.1, gamma=0.99,\n",
    "                       epsilon_start=1.0, epsilon_end=0.1, decay_rate=0.9995):\n",
    "\n",
    "    global stations, candidates_p,candidates_goal, pickup\n",
    "    stations = [[0,0] for _ in range(4)]\n",
    "    candidates_p = [i for i in stations]\n",
    "    candidates_goal = [i for i in stations]\n",
    "    pickup=False\n",
    "    action_size = 6\n",
    "    q_table = defaultdict(lambda :np.zeros(action_size))\n",
    "    pickup_id = 4\n",
    "    drop_id = 5\n",
    "    ifpickup=False\n",
    "    p_loc = (0,0)\n",
    "    def cmp(a,b):\n",
    "        if a==b:\n",
    "            return 0\n",
    "        return 1 if a<b else -1\n",
    "    def get_state_obs(obs,action,last_action=None):\n",
    "        global stations,pickup,candidates_p,candidates_goal\n",
    "        #print(candidates_p)\n",
    "        taxi_row, taxi_col, stations[0][0], stations[0][1] , stations[1][0], stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "        agent_pos = (taxi_row,taxi_col)\n",
    "        if action==None:\n",
    "            # initialize\n",
    "            candidates_goal = [tuple(i) for i in stations]\n",
    "            candidates_p = [tuple(i) for i in stations]\n",
    "            pickup=False\n",
    "        if passenger_look:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        else:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        if destination_look:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        else:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        if action==pickup_id and not pickup and agent_pos in candidates_p:\n",
    "            pickup = True\n",
    "            candidates_p = []\n",
    "        elif action == drop_id and pickup:\n",
    "            pickup=False\n",
    "            candidates_p.append(agent_pos)\n",
    "        cmp_pos = (0,0)\n",
    "        if not pickup:\n",
    "            cmp_pos = candidates_p[0]\n",
    "        else:\n",
    "            cmp_pos = candidates_goal[0]\n",
    "        passenger_look = passenger_look and agent_pos in candidates_p\n",
    "        destination_look = destination_look and agent_pos in candidates_goal\n",
    "        relative_pos = (cmp(agent_pos[0],cmp_pos[0]),cmp(agent_pos[1],cmp_pos[1]))\n",
    "        return (relative_pos,pickup, passenger_look, destination_look, (obstacle_north,obstacle_south,obstacle_east,obstacle_west),last_action)\n",
    "    \"\"\"\n",
    "    if action == 0 :  # Move Down\n",
    "        next_row += 1\n",
    "    elif action == 1:  # Move Up\n",
    "        next_row -= 1\n",
    "    elif action == 2:  # Move Right\n",
    "        next_col += 1\n",
    "    elif action == 3:  # Move Left\n",
    "        next_col -= 1\n",
    "    \"\"\"\n",
    "    station_size = 4\n",
    "    total_reward = 0\n",
    "    total_reward_shaped = 0\n",
    "    cnt = [0,0,0,0]\n",
    "    epsilon = epsilon_start\n",
    "    averaged = [0,0]\n",
    "    batch_size = 100\n",
    "    for epoch in tqdm(range(episodes+batch_size)):\n",
    "        if epoch >=episodes:\n",
    "            epsilon = 0\n",
    "        grid_size = 5 #np.random.randint(5,11)\n",
    "        obstacle_size = np.random.randint(grid_size*grid_size)\n",
    "        obs,_ = env.reset(grid_size,obstacle_size)\n",
    "        averaged[0]+=env.grid_size\n",
    "        averaged[1]+=len(env.obstacles)\n",
    "        obs,ifpickup,p_loc = obs\n",
    "        done = False\n",
    "        state = get_state_obs(obs,action=None)\n",
    "        steps=0\n",
    "        action_l=[]\n",
    "        success = False\n",
    "        has_pickup=False\n",
    "\n",
    "        while not done:\n",
    "            \"\"\"if state[-1]!=(0,0,0,0):\n",
    "              print(state[-1])\"\"\"\n",
    "            if np.random.choice(2,p=[epsilon,1-epsilon])==0 or state not in q_table.keys():\n",
    "                action = np.random.randint(6)\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "            lst_pickup = pickup\n",
    "            \n",
    "            relative_pos,pickup, _,_ , _,last_action = state\n",
    "            obs,reward,done,_ = env.step(action)\n",
    "            obs,ifpickup,p_loc = obs\n",
    "            if action in [0,1,2,3]:\n",
    "                next_state = get_state_obs(obs,action,action)\n",
    "            else:\n",
    "                next_state = get_state_obs(obs,action,last_action)\n",
    "            total_reward += reward\n",
    "\n",
    "            ### reward shaping\n",
    "            reward_shaping = 0\n",
    "            \"\"\"if relative_pos == (0,0):\n",
    "                # want it to go to possible goal\n",
    "                #reward_shaping += 0.5\"\"\"\n",
    "            if relative_pos != (0,0) and action in [pickup_id,drop_id]:\n",
    "                reward_shaping -= 10\n",
    "            if done and reward>0:\n",
    "                cnt[2]+=1\n",
    "            \n",
    "            relative_pos,pickup, _,_ , _,_ = next_state\n",
    "            if not done and reward <-10 and action in [0,1,2,3]:\n",
    "                #print('hit wall')\n",
    "                cnt[-1]+=1\n",
    "            \"\"\"if not pickup:\n",
    "                reward_shaping -= p_len\n",
    "            reward_shaping -= 0.1 * goal_len\"\"\"\n",
    "            reward += reward_shaping\n",
    "            total_reward_shaped += reward\n",
    "            if epsilon:\n",
    "                q_table[state][action] = q_table[state][action] + alpha*(reward+gamma*np.max(q_table[next_state])-q_table[state][action])\n",
    "            state = next_state\n",
    "            if lst_pickup==False and pickup:\n",
    "                cnt[0]+=1\n",
    "            elif lst_pickup==True and pickup==False:\n",
    "                cnt[1]+=1\n",
    "            if pickup!=ifpickup:\n",
    "                print(pickup,ifpickup)\n",
    "            assert(pickup==ifpickup)\n",
    "\n",
    "\n",
    "        if (epoch+1)%batch_size==0:\n",
    "            cnt = [i/batch_size for i in cnt]\n",
    "            print(f'Epsilon : {epsilon}, average reward : {total_reward/batch_size:.4f}, averaged shaped reward : {total_reward_shaped/batch_size:.4f} Pickup, Drop, Success, Hit wall rate : {cnt}')\n",
    "            print(f'averaged grid size : {averaged[0]/batch_size:.2f}, averaged obstacles : {averaged[1]/batch_size:.2f}')\n",
    "            averaged = [0,0]\n",
    "            cnt = [0,0,0,0]\n",
    "            total_reward = 0\n",
    "            total_reward_shaped = 0\n",
    "        epsilon *= decay_rate #max(epsilon*decay_rate ,epsilon_end)\n",
    "    return q_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oxVEy1uSuNb",
    "outputId": "137a8cf5-0a6b-4e72-9b7c-f5a5c8dddc9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 99/5100 [00:05<03:41, 22.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.9516933769307994, average reward : -10187.0830, averaged shaped reward : -15210.5830 Pickup, Drop, Success, Hit wall rate : [25.31, 25.3, 0.92, 505.52]\n",
      "averaged grid size : 5.00, averaged obstacles : 11.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 204/5100 [00:08<02:03, 39.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.9052674235521029, average reward : -4137.3180, averaged shaped reward : -6266.7180 Pickup, Drop, Success, Hit wall rate : [17.48, 17.48, 0.99, 203.6]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 305/5100 [00:10<01:17, 62.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.8611062428400729, average reward : -2672.6250, averaged shaped reward : -3976.7250 Pickup, Drop, Success, Hit wall rate : [15.56, 15.56, 1.0, 140.91]\n",
      "averaged grid size : 5.00, averaged obstacles : 11.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 411/5100 [00:12<01:15, 62.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.8190993535905904, average reward : -1945.2600, averaged shaped reward : -2882.1600 Pickup, Drop, Success, Hit wall rate : [14.25, 14.25, 0.99, 106.91]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 521/5100 [00:13<00:53, 85.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.7791416641455342, average reward : -1353.0670, averaged shaped reward : -2026.8670 Pickup, Drop, Success, Hit wall rate : [11.62, 11.62, 1.0, 75.85]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 614/5100 [00:14<00:47, 93.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.7411332094774175, average reward : -819.8240, averaged shaped reward : -1249.8240 Pickup, Drop, Success, Hit wall rate : [8.69, 8.69, 1.0, 46.79]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 727/5100 [00:15<00:46, 94.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.7049789010996835, average reward : -848.3940, averaged shaped reward : -1275.2940 Pickup, Drop, Success, Hit wall rate : [9.78, 9.78, 1.0, 49.79]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 823/5100 [00:16<00:38, 112.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.6705882891769959, average reward : -469.1880, averaged shaped reward : -723.6880 Pickup, Drop, Success, Hit wall rate : [6.71, 6.71, 1.0, 28.61]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 913/5100 [00:17<00:44, 94.68it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.6378753362403742, average reward : -797.2480, averaged shaped reward : -1226.4480 Pickup, Drop, Success, Hit wall rate : [9.91, 9.91, 1.0, 45.55]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1006/5100 [00:18<00:44, 91.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.6067582019410674, average reward : -435.3620, averaged shaped reward : -673.0620 Pickup, Drop, Success, Hit wall rate : [6.53, 6.53, 1.0, 27.12]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1118/5100 [00:19<00:36, 108.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.5771590383046616, average reward : -866.2100, averaged shaped reward : -1320.7100 Pickup, Drop, Success, Hit wall rate : [9.87, 9.87, 1.0, 49.02]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 1206/5100 [00:20<00:36, 107.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.5490037949732016, average reward : -377.8890, averaged shaped reward : -599.4890 Pickup, Drop, Success, Hit wall rate : [6.71, 6.71, 1.0, 23.26]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1317/5100 [00:21<00:36, 103.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.5222220339480774, average reward : -398.3320, averaged shaped reward : -629.5320 Pickup, Drop, Success, Hit wall rate : [6.77, 6.77, 1.0, 24.38]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1409/5100 [00:22<00:34, 106.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.49674675337021873, average reward : -346.9480, averaged shaped reward : -547.1480 Pickup, Drop, Success, Hit wall rate : [5.87, 5.87, 1.0, 22.48]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1508/5100 [00:23<00:41, 85.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.47251421989671744, average reward : -644.7170, averaged shaped reward : -1007.6170 Pickup, Drop, Success, Hit wall rate : [6.93, 6.92, 0.99, 36.47]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 1605/5100 [00:24<00:35, 98.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.44946380925453877, average reward : -416.5330, averaged shaped reward : -658.4330 Pickup, Drop, Success, Hit wall rate : [7.67, 7.67, 1.0, 25.91]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1706/5100 [00:25<00:38, 89.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.4275378545724137, average reward : -461.7670, averaged shaped reward : -733.2670 Pickup, Drop, Success, Hit wall rate : [8.69, 8.69, 1.0, 28.48]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1830/5100 [00:26<00:25, 129.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.4066815021114777, average reward : -356.3780, averaged shaped reward : -559.1780 Pickup, Drop, Success, Hit wall rate : [3.82, 3.82, 1.0, 21.45]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1926/5100 [00:27<00:25, 125.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.38684257403372235, average reward : -296.1850, averaged shaped reward : -481.4850 Pickup, Drop, Success, Hit wall rate : [4.12, 4.12, 1.0, 17.38]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2020/5100 [00:28<00:20, 147.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.3679714378649446, average reward : -142.0520, averaged shaped reward : -247.0520 Pickup, Drop, Success, Hit wall rate : [2.64, 2.64, 1.0, 9.27]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2121/5100 [00:29<00:19, 149.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.35002088232561296, average reward : -156.0190, averaged shaped reward : -264.2190 Pickup, Drop, Success, Hit wall rate : [3.15, 3.15, 1.0, 10.47]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2217/5100 [00:29<00:21, 133.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.33294599921901236, average reward : -151.7240, averaged shaped reward : -271.5240 Pickup, Drop, Success, Hit wall rate : [4.19, 4.19, 1.0, 10.13]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2327/5100 [00:30<00:20, 135.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.3167040710811749, average reward : -120.3130, averaged shaped reward : -212.5130 Pickup, Drop, Success, Hit wall rate : [3.39, 3.39, 1.0, 8.88]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 2399/5100 [00:31<00:20, 133.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.3012544643115281, average reward : -198.0770, averaged shaped reward : -324.0770 Pickup, Drop, Success, Hit wall rate : [1.9, 1.9, 1.0, 12.03]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2520/5100 [00:32<00:23, 111.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.2865585275168893, average reward : -173.6340, averaged shaped reward : -292.7340 Pickup, Drop, Success, Hit wall rate : [3.29, 3.29, 1.0, 11.0]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2627/5100 [00:33<00:20, 118.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.2725794948144954, average reward : -100.0120, averaged shaped reward : -184.8120 Pickup, Drop, Success, Hit wall rate : [3.13, 3.13, 1.0, 7.16]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 2703/5100 [00:34<00:16, 146.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.2592823938521472, average reward : -34.8170, averaged shaped reward : -76.1170 Pickup, Drop, Success, Hit wall rate : [1.6, 1.6, 1.0, 4.47]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2822/5100 [00:35<00:20, 111.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.2466339583153596, average reward : -240.1240, averaged shaped reward : -383.9240 Pickup, Drop, Success, Hit wall rate : [2.27, 2.27, 1.0, 13.96]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 2921/5100 [00:35<00:15, 139.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.23460254470262715, average reward : -68.5880, averaged shaped reward : -128.7880 Pickup, Drop, Success, Hit wall rate : [2.04, 2.04, 1.0, 5.86]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 3021/5100 [00:36<00:13, 150.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.22315805316059978, average reward : -36.5160, averaged shaped reward : -81.3160 Pickup, Drop, Success, Hit wall rate : [2.07, 2.07, 1.0, 4.49]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3115/5100 [00:37<00:16, 120.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.21227185218111316, average reward : -95.9540, averaged shaped reward : -173.0540 Pickup, Drop, Success, Hit wall rate : [1.75, 1.75, 1.0, 6.68]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3203/5100 [00:38<00:16, 117.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.20191670697168435, average reward : -50.8390, averaged shaped reward : -105.4390 Pickup, Drop, Success, Hit wall rate : [2.5, 2.5, 1.0, 5.29]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 3317/5100 [00:38<00:10, 172.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.19206671132027073, average reward : -62.8730, averaged shaped reward : -119.3730 Pickup, Drop, Success, Hit wall rate : [1.7, 1.7, 1.0, 5.5]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 3409/5100 [00:39<00:18, 92.63it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.1826972227838353, average reward : -79.9300, averaged shaped reward : -153.4300 Pickup, Drop, Success, Hit wall rate : [2.73, 2.73, 1.0, 6.28]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3519/5100 [00:40<00:10, 154.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.1737848010385734, average reward : -21.8850, averaged shaped reward : -53.0850 Pickup, Drop, Success, Hit wall rate : [1.48, 1.48, 1.0, 3.82]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3629/5100 [00:41<00:09, 148.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.1653071492375671, average reward : -19.4960, averaged shaped reward : -52.9960 Pickup, Drop, Success, Hit wall rate : [1.36, 1.36, 1.0, 3.31]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 3716/5100 [00:42<00:09, 151.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.15724305822915946, average reward : -8.4680, averaged shaped reward : -38.4680 Pickup, Drop, Success, Hit wall rate : [1.33, 1.33, 1.0, 2.57]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 3818/5100 [00:42<00:10, 120.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.14957235349649245, average reward : -50.2960, averaged shaped reward : -99.1960 Pickup, Drop, Success, Hit wall rate : [1.92, 1.92, 1.0, 5.19]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 3919/5100 [00:44<00:12, 91.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.14227584468546683, average reward : -114.2060, averaged shaped reward : -204.3060 Pickup, Drop, Success, Hit wall rate : [2.0, 2.0, 1.0, 6.63]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4020/5100 [00:44<00:08, 126.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.13533527759485164, average reward : -13.9270, averaged shaped reward : -44.2270 Pickup, Drop, Success, Hit wall rate : [1.27, 1.27, 1.0, 2.93]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4102/5100 [00:46<00:12, 78.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.12873328850843568, average reward : -36.4170, averaged shaped reward : -87.7170 Pickup, Drop, Success, Hit wall rate : [2.46, 2.46, 1.0, 3.89]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4210/5100 [00:47<00:10, 88.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.12245336075496815, average reward : 20.2130, averaged shaped reward : 4.8130 Pickup, Drop, Success, Hit wall rate : [1.41, 1.41, 1.0, 1.56]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 4317/5100 [00:49<00:09, 80.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.11647978338721439, average reward : -64.7280, averaged shaped reward : -117.0280 Pickup, Drop, Success, Hit wall rate : [1.33, 1.33, 1.0, 5.34]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 4415/5100 [00:51<00:12, 56.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.11079761187674808, average reward : -61.5730, averaged shaped reward : -115.6730 Pickup, Drop, Success, Hit wall rate : [2.03, 2.03, 1.0, 5.29]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 4506/5100 [00:52<00:08, 71.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.10539263072614893, average reward : -21.5140, averaged shaped reward : -54.7140 Pickup, Drop, Success, Hit wall rate : [1.38, 1.38, 1.0, 3.22]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4599/5100 [00:54<00:08, 61.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.10025131790506973, average reward : -30.4830, averaged shaped reward : -68.2830 Pickup, Drop, Success, Hit wall rate : [2.09, 2.09, 1.0, 4.1]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 4706/5100 [00:55<00:04, 79.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.09536081102119956, average reward : -24.0040, averaged shaped reward : -60.4040 Pickup, Drop, Success, Hit wall rate : [1.8, 1.8, 1.0, 3.55]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 4818/5100 [00:57<00:03, 73.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.09070887514149145, average reward : -29.5330, averaged shaped reward : -69.8330 Pickup, Drop, Success, Hit wall rate : [1.27, 1.27, 1.0, 3.17]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 4922/5100 [00:58<00:02, 71.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.08628387218314981, average reward : -53.8850, averaged shaped reward : -100.4850 Pickup, Drop, Success, Hit wall rate : [1.47, 1.47, 0.99, 4.38]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 5001/5100 [00:59<00:01, 62.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.082074731797801, average reward : 6.0460, averaged shaped reward : -15.1540 Pickup, Drop, Success, Hit wall rate : [1.19, 1.19, 1.0, 1.68]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5100/5100 [01:04<00:00, 78.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0, average reward : -80.0120, averaged shaped reward : -80.0120 Pickup, Drop, Success, Hit wall rate : [0.82, 0.77, 0.77, 0.01]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = SimpleTaxiEnv()\n",
    "q_table = tabular_q_learning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "FaVsGr6fbI7l"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "q_table_dict = dict(q_table)  # Convert to regular dict\n",
    "with open('q_table.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "1yAJ9k3vWNPX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304\n"
     ]
    }
   ],
   "source": [
    "print(len(q_table_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXo_X3thVos1"
   },
   "source": [
    "# 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def Testing(env):\n",
    "    import pickle\n",
    "    from collections import defaultdict\n",
    "    with open('q_table.pkl', 'rb') as f:\n",
    "        print('load')\n",
    "        loaded_dict = pickle.load(f)\n",
    "    q_table = defaultdict(lambda: np.zeros(6), loaded_dict)  # Replace 0 with your default value\n",
    "    print(len(q_table))\n",
    "    #print('len of q_table',len(q_table.keys()))\n",
    "    global stations, candidates_p,candidates_goal, pickup, goal_id,last_action,last_record_action\n",
    "    stations = [[0,0] for _ in range(4)]\n",
    "    candidates_p = [i for i in stations]\n",
    "    candidates_goal = [i for i in stations]\n",
    "    goal_id = -1\n",
    "    pickup=False\n",
    "    action_size = 6\n",
    "    last_action = None\n",
    "    last_record_action = None\n",
    "    #q_table = defaultdict(lambda :np.zeros(action_size))\n",
    "    pickup_id = 4\n",
    "    drop_id = 5\n",
    "    ifpickup=False\n",
    "    p_loc = (0,0)\n",
    "    def cmp(a,b):\n",
    "        if a==b:\n",
    "            return 0\n",
    "        return 1 if a<b else -1\n",
    "            \n",
    "    def get_state_obs(obs,action,last_action=None):\n",
    "        global stations,pickup,candidates_p,candidates_goal\n",
    "        #print(candidates_p)\n",
    "        taxi_row, taxi_col, stations[0][0], stations[0][1] , stations[1][0], stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "        agent_pos = (taxi_row,taxi_col)\n",
    "        if action==None:\n",
    "            # initialize\n",
    "            candidates_goal = [tuple(i) for i in stations]\n",
    "            candidates_p = [tuple(i) for i in stations]\n",
    "            pickup=False\n",
    "        if passenger_look:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        else:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        if destination_look:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        else:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        if action==pickup_id and not pickup and agent_pos in candidates_p:\n",
    "            pickup = True\n",
    "            candidates_p = []\n",
    "        elif action == drop_id and pickup:\n",
    "            pickup=False\n",
    "            candidates_p.append(agent_pos)\n",
    "        cmp_pos = (0,0)\n",
    "        if not pickup:\n",
    "            cmp_pos = candidates_p[0]\n",
    "        else:\n",
    "            cmp_pos = candidates_goal[0]\n",
    "        passenger_look = passenger_look and agent_pos in candidates_p\n",
    "        destination_look = destination_look and agent_pos in candidates_goal\n",
    "        relative_pos = (cmp(agent_pos[0],cmp_pos[0]),cmp(agent_pos[1],cmp_pos[1]))\n",
    "        return (relative_pos,pickup, passenger_look, destination_look, (obstacle_north,obstacle_south,obstacle_east,obstacle_west),last_action)\n",
    "\n",
    "    def get_action(obs):\n",
    "        # TODO: Train your own agent\n",
    "        # HINT: If you're using a Q-table, consider designing a custom key based on `obs` to store useful information.\n",
    "        # NOTE: Keep in mind that your Q-table may not cover all possible states in the testing environment.\n",
    "        #       To prevent crashes, implement a fallback strategy for missing keys.\n",
    "        #       Otherwise, even if your agent performs well in training, it may fail during testing.\n",
    "        global last_action,last_record_action\n",
    "        state = get_state_obs(obs,last_action,last_record_action)\n",
    "        action_name = ['Move North','Move South','Move East','Move West','Pick Up','Drop Off']\n",
    "        if state not in q_table.keys():\n",
    "            #print(state)\n",
    "            print(state)\n",
    "            assert(0)\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            #print(state,action_name[np.argmax(q_table[state])])\n",
    "            action = np.argmax(q_table[state])\n",
    "        last_action = action\n",
    "        if action in [0,1,2,3]:\n",
    "            last_record_action = action\n",
    "        #q_table[state][action] = q_table[state][action] + 0.089487*(-0.1+0.89487*np.max(q_table[state])-q_table[state][action])\n",
    "        return action # Choose a random action\n",
    "        # You can submit this random agent to evaluate the performance of a purely random strategy.\n",
    "    Total_reward=0\n",
    "    for i in tqdm(range(100)):\n",
    "        grid_size = np.random.randint(5,11)\n",
    "        obstacle_size = np.random.randint(grid_size*grid_size)\n",
    "        obs,_ = env.reset()\n",
    "        obs,_,_ = obs\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        stations = [[0,0] for _ in range(4)]\n",
    "        candidates_p = [i for i in stations]\n",
    "        candidates_goal = [i for i in stations]\n",
    "        goal_id = -1\n",
    "        pickup=False\n",
    "        action_size = 6\n",
    "        pickup_id = 4\n",
    "        drop_id = 5\n",
    "        last_action = None\n",
    "        last_record_action = None\n",
    "        while not done:\n",
    "            action = get_action(obs)\n",
    "            obs,reward,done,_ = env.step(action)\n",
    "            obs,_,_ = obs\n",
    "            total_reward += reward\n",
    "        Total_reward+=total_reward\n",
    "        print(f'grid_size : {env.grid_size}, obstacle_size : {len(env.obstacles)}, total_reward : {total_reward}')\n",
    "    print(f'average : {Total_reward/100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:03, 27.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 7, obstacle_size : 17, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 31, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 36, total_reward : 47.8\n",
      "grid_size : 6, obstacle_size : 25, total_reward : 48.4\n",
      "grid_size : 8, obstacle_size : 1, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 3, total_reward : 48.699999999999996\n",
      "grid_size : 7, obstacle_size : 32, total_reward : -510.0000000000452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:02, 31.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "grid_size : 7, obstacle_size : 21, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 23, total_reward : -510.0000000000452\n",
      "grid_size : 9, obstacle_size : 29, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 47, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:00<00:03, 24.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 7, obstacle_size : 36, total_reward : 46.4\n",
      "grid_size : 5, obstacle_size : 16, total_reward : 49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:00<00:04, 20.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 78, total_reward : 49.0\n",
      "grid_size : 10, obstacle_size : 28, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 17, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 38, total_reward : 48.3\n",
      "grid_size : 7, obstacle_size : 3, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 4, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:00<00:03, 21.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 25, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 21, total_reward : 47.699999999999996\n",
      "grid_size : 7, obstacle_size : 1, total_reward : 48.1\n",
      "grid_size : 9, obstacle_size : 38, total_reward : 48.4\n",
      "grid_size : 5, obstacle_size : 4, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 0, total_reward : 48.5\n",
      "grid_size : 6, obstacle_size : 24, total_reward : 48.699999999999996\n",
      "grid_size : 5, obstacle_size : 11, total_reward : 48.1\n",
      "grid_size : 8, obstacle_size : 43, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:01<00:02, 29.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 6, obstacle_size : 18, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 26, total_reward : 49.1\n",
      "grid_size : 6, obstacle_size : 19, total_reward : 48.699999999999996\n",
      "grid_size : 7, obstacle_size : 36, total_reward : 48.699999999999996\n",
      "grid_size : 9, obstacle_size : 19, total_reward : 48.0\n",
      "grid_size : 8, obstacle_size : 17, total_reward : 47.9\n",
      "grid_size : 8, obstacle_size : 18, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:01<00:02, 31.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 7, obstacle_size : 33, total_reward : 48.3\n",
      "grid_size : 9, obstacle_size : 44, total_reward : -510.0000000000452\n",
      "grid_size : 9, obstacle_size : 18, total_reward : 47.3\n",
      "grid_size : 8, obstacle_size : 40, total_reward : 46.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:01<00:02, 26.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 72, total_reward : 47.5\n",
      "grid_size : 8, obstacle_size : 21, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 22, total_reward : 47.9\n",
      "grid_size : 10, obstacle_size : 4, total_reward : 47.8\n",
      "grid_size : 9, obstacle_size : 2, total_reward : 46.199999999999996\n",
      "grid_size : 9, obstacle_size : 42, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 3, total_reward : 48.5\n",
      "grid_size : 7, obstacle_size : 5, total_reward : 47.099999999999994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:02<00:02, 24.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 72, total_reward : 45.699999999999996\n",
      "grid_size : 7, obstacle_size : 20, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 12, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 16, total_reward : -520.0000000000474\n",
      "grid_size : 9, obstacle_size : 16, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 22, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:02<00:01, 24.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 6, obstacle_size : 21, total_reward : 48.5\n",
      "grid_size : 5, obstacle_size : 5, total_reward : 49.3\n",
      "grid_size : 8, obstacle_size : 4, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 11, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 52, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [00:02<00:00, 32.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 8, obstacle_size : 46, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 2, total_reward : 46.8\n",
      "grid_size : 8, obstacle_size : 18, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 9, total_reward : 48.6\n",
      "grid_size : 5, obstacle_size : 16, total_reward : 48.1\n",
      "grid_size : 6, obstacle_size : 21, total_reward : 48.199999999999996\n",
      "grid_size : 7, obstacle_size : 25, total_reward : 47.8\n",
      "grid_size : 7, obstacle_size : 15, total_reward : 47.599999999999994\n",
      "grid_size : 6, obstacle_size : 21, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 14, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:02<00:00, 28.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 8, obstacle_size : 40, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 68, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 14, total_reward : 49.3\n",
      "grid_size : 7, obstacle_size : 1, total_reward : 46.3\n",
      "grid_size : 10, obstacle_size : 46, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [00:03<00:01, 22.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 7, obstacle_size : 28, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 25, total_reward : 49.1\n",
      "grid_size : 9, obstacle_size : 59, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 42, total_reward : 48.8\n",
      "grid_size : 7, obstacle_size : 18, total_reward : 38.4\n",
      "grid_size : 5, obstacle_size : 18, total_reward : 49.0\n",
      "grid_size : 8, obstacle_size : 4, total_reward : 47.9\n",
      "grid_size : 7, obstacle_size : 32, total_reward : 48.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:03<00:00, 27.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 9, obstacle_size : 38, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 37, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 47, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:03<00:00, 24.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 8, obstacle_size : 53, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 3, total_reward : 48.3\n",
      "grid_size : 5, obstacle_size : 5, total_reward : 49.3\n",
      "grid_size : 9, obstacle_size : 45, total_reward : 46.099999999999994\n",
      "grid_size : 10, obstacle_size : 64, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [00:03<00:00, 22.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 9, obstacle_size : 48, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 2, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 12, total_reward : 48.5\n",
      "grid_size : 5, obstacle_size : 17, total_reward : 48.8\n",
      "grid_size : 10, obstacle_size : 52, total_reward : 49.199999999999996\n",
      "grid_size : 9, obstacle_size : 44, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 10, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 32, total_reward : 47.699999999999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 24.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 8, obstacle_size : 41, total_reward : -510.0000000000452\n",
      "grid_size : 9, obstacle_size : 19, total_reward : 48.199999999999996\n",
      "grid_size : 7, obstacle_size : 33, total_reward : 47.599999999999994\n",
      "average : -214.4170000000212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = SimpleTaxiEnv()\n",
    "Testing(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSxQEFroMcJw"
   },
   "source": [
    "# Policy base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "nP4NtkKwMi1s"
   },
   "outputs": [],
   "source": [
    "def tabular_policy_learning(env,episodes=5000, alpha=0.1, gamma=0.999):\n",
    "\n",
    "    global stations, candidates_p,candidates_goal, pickup\n",
    "    stations = [[0,0] for _ in range(4)]\n",
    "    candidates_p = [i for i in stations]\n",
    "    candidates_goal = [i for i in stations]\n",
    "    pickup=False\n",
    "    action_size = 6\n",
    "    policy_table = defaultdict(lambda :np.zeros(action_size))\n",
    "    pickup_id = 4\n",
    "    drop_id = 5\n",
    "    ifpickup=False\n",
    "    p_loc = (0,0)\n",
    "    def softmax(x):\n",
    "        \"\"\"✅ Compute softmax values for an array.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))  # Numeric stability\n",
    "        return exp_x / exp_x.sum()\n",
    "    def get_state_obs(obs,action):\n",
    "        global stations,pickup,candidates_p,candidates_goal\n",
    "        #print(candidates_p)\n",
    "        obstacles = [0 for _ in range(5)]\n",
    "        taxi_row, taxi_col, stations[0][0], stations[0][1] , stations[1][0], stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "        agent_pos = (taxi_row,taxi_col)\n",
    "        if action==None:\n",
    "            # initialize\n",
    "            candidates_goal = [tuple(i) for i in stations]\n",
    "            candidates_p = [tuple(i) for i in stations]\n",
    "            pickup=False\n",
    "        if passenger_look:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        else:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        if destination_look:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        else:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        if action==pickup_id and not pickup and agent_pos in candidates_p:\n",
    "            pickup = True\n",
    "            candidates_p = []\n",
    "        elif action == drop_id and pickup:\n",
    "            pickup=False\n",
    "            candidates_p = [agent_pos]\n",
    "        return (pickup, len(candidates_p), len(candidates_goal), passenger_look, destination_look, (obstacle_north,obstacle_south,obstacle_east,obstacle_west))\n",
    "    \"\"\"\n",
    "    if action == 0 :  # Move Down\n",
    "        next_row += 1\n",
    "    elif action == 1:  # Move Up\n",
    "        next_row -= 1\n",
    "    elif action == 2:  # Move Right\n",
    "        next_col += 1\n",
    "    elif action == 3:  # Move Left\n",
    "        next_col -= 1\n",
    "    \"\"\"\n",
    "    station_size = 4\n",
    "    total_reward = 0\n",
    "    total_reward_shaped = 0\n",
    "    cnt = [0,0,0]\n",
    "    averaged = [0,0]\n",
    "    batch_size = 100\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(episodes+600)):\n",
    "        if epoch >= episodes:\n",
    "            epsilon = 0\n",
    "        grid_size = np.random.randint(5,11)\n",
    "        obstacle_size = np.random.randint(grid_size*grid_size)\n",
    "        obs,_ = env.reset()\n",
    "        averaged[0]+=env.grid_size\n",
    "        averaged[1]+=len(env.obstacles)\n",
    "        obs,ifpickup,p_loc = obs\n",
    "        done = False\n",
    "        state = get_state_obs(obs,action=None)\n",
    "        steps=0\n",
    "        action_l=[]\n",
    "        success = False\n",
    "        has_pickup=False\n",
    "        trajectory = []\n",
    "        while not done:\n",
    "            \"\"\"if state[-1]!=(0,0,0,0):\n",
    "              print(state[-1])\"\"\"\n",
    "            action = np.random.choice(action_size,p=softmax(policy_table[state]))\n",
    "            lst_pickup = pickup\n",
    "            obs,reward,done,_ = env.step(action)\n",
    "            obs,ifpickup,p_loc = obs\n",
    "            next_state = get_state_obs(obs,action)\n",
    "            total_reward += reward\n",
    "            if done and reward>0:\n",
    "                cnt[2]+=1\n",
    "            reward_shaping = 0\n",
    "            pickup, p_len, goal_len, _,_ , _ = next_state\n",
    "            if not pickup:\n",
    "                reward_shaping -= 0.01 * (p_len**2)\n",
    "            reward_shaping -= 0.001 * (goal_len**2)\n",
    "            reward += reward_shaping\n",
    "            total_reward_shaped += reward\n",
    "            trajectory.append((state,action,reward))\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "            if lst_pickup==False and pickup:\n",
    "                cnt[0]+=1\n",
    "            elif lst_pickup==True and pickup==False:\n",
    "                cnt[1]+=1\n",
    "            if pickup!=ifpickup:\n",
    "                print(pickup,ifpickup)\n",
    "            assert(pickup==ifpickup)\n",
    "        G = 0\n",
    "        for t in reversed(trajectory):\n",
    "            state,action,reward = t\n",
    "            G = gamma*G + reward\n",
    "            prob = softmax(policy_table[state])\n",
    "            grad=-prob.copy()\n",
    "            grad[action]+=1\n",
    "            policy_table[state] += alpha*G*grad\n",
    "        if (epoch+1)%batch_size==0:\n",
    "            cnt = [i/batch_size for i in cnt]\n",
    "            print(f'Epoch : {epoch}, average reward : {total_reward/batch_size:.4f}, averaged shaped reward : {total_reward_shaped/batch_size:.4f} Pickup, Drop, Success rate : {cnt}')\n",
    "            print(f'averaged grid size : {averaged[0]/batch_size:.2f}, averaged obstacles : {averaged[1]/batch_size:.2f}')\n",
    "            averaged = [0,0]\n",
    "            cnt = [0,0,0]\n",
    "            total_reward = 0\n",
    "            total_reward_shaped = 0\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8pe7Sg6NarrP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/5600 [00:04<20:19,  4.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m SimpleTaxiEnv()\n\u001b[1;32m----> 2\u001b[0m policy_table \u001b[38;5;241m=\u001b[39m \u001b[43mtabular_policy_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# save policy table to .pkl\u001b[39;00m\n\u001b[0;32m      4\u001b[0m policy_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(policy_table)  \u001b[38;5;66;03m# Convert to regular dict\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 120\u001b[0m, in \u001b[0;36mtabular_policy_learning\u001b[1;34m(env, episodes, alpha, gamma)\u001b[0m\n\u001b[0;32m    118\u001b[0m state,action,reward \u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m    119\u001b[0m G \u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m*\u001b[39mG \u001b[38;5;241m+\u001b[39m reward\n\u001b[1;32m--> 120\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mprob\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    122\u001b[0m grad[action]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[44], line 17\u001b[0m, in \u001b[0;36mtabular_policy_learning.<locals>.softmax\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"✅ Compute softmax values for an array.\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m exp_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(x \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x))  \u001b[38;5;66;03m# Numeric stability\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp_x \u001b[38;5;241m/\u001b[39m \u001b[43mexp_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\_core\\_methods.py:51\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = SimpleTaxiEnv()\n",
    "policy_table = tabular_policy_learning(env)\n",
    "# save policy table to .pkl\n",
    "policy_dict = dict(policy_table)  # Convert to regular dict\n",
    "with open('policy_table.pkl', 'wb') as f:\n",
    "    pickle.dump(policy_dict, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vBm02Hg7kQwD",
    "ioWeQB4DTncX"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
