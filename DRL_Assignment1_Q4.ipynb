{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fwkxPaZaMFv"
   },
   "source": [
    "# **Deep Reinforcement Learning Class Spring 2025 Assignment 1**\n",
    "\n",
    "In this assignment, we will learn about gym interface, gridworld, q-learning, and etc. You will need to fill in the missing code snippets (marked by TODO).\n",
    "\n",
    "Make a copy of this notebook using File > Save a copy in Drive and edit it with your answers.\n",
    "\n",
    "WARNING: Do not put your name or any other personal identification information in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gym) (2.1.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\yuhun\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833987E30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833E45F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833E46300>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833E46510>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001C833E466F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "ERROR: Could not find a version that satisfies the requirement numpy==1.24.0 (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\yuhun\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for numpy==1.24.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\yuhun\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\yuhun\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install numpy==1.24.0\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L8nLFy4z2bZh"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import importlib.util\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W_kyHQESMG0E"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "suK5e6_R2Fcq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This environment allows you to verify whether your program runs correctly during testing,\n",
    "# as it follows the same observation format from `env.reset()` and `env.step()`.\n",
    "# However, keep in mind that this is just a simplified environment.\n",
    "# The full specifications for the real testing environment can be found in the provided spec.\n",
    "#\n",
    "# You are free to modify this file to better match the real environment and train your own agent.\n",
    "# Good luck!\n",
    "\n",
    "\n",
    "class SimpleTaxiEnv(gym.Env):\n",
    "    def __init__(self, grid_size=10, fuel_limit=5000):\n",
    "        \"\"\"\n",
    "        Custom Taxi environment supporting different grid sizes.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.fuel_limit = fuel_limit\n",
    "        self.current_fuel = fuel_limit\n",
    "        self.passenger_picked_up = False\n",
    "\n",
    "        self.stations = [(0, 0), (0, self.grid_size - 1), (self.grid_size - 1, 0), (self.grid_size - 1, self.grid_size - 1)]\n",
    "        self.passenger_loc = None\n",
    "\n",
    "        self.obstacles = set()  # No obstacles in simple version\n",
    "        self.destination = None\n",
    "    def legal(self):\n",
    "      available_positions = [\n",
    "            (x, y) for x in range(self.grid_size) for y in range(self.grid_size)\n",
    "            if (x, y) not in self.obstacles\n",
    "      ]\n",
    "      n=len(available_positions)\n",
    "      p = [0 for i in range(n)]\n",
    "      global group_size\n",
    "      group_size = n\n",
    "      def find_parent(x):\n",
    "        if x!=p[x]:\n",
    "          p[x]=find_parent(p[x])\n",
    "        return p[x]\n",
    "      def Union(a,b):\n",
    "        global group_size\n",
    "        a=find_parent(a)\n",
    "        b=find_parent(b)\n",
    "        if a!=b:\n",
    "          group_size -= 1\n",
    "          if np.random.randint(2)==0:\n",
    "            p[a]=b\n",
    "          else:\n",
    "            p[b]=a\n",
    "      for i in range(n):\n",
    "        p[i]=i\n",
    "      for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "          if abs(available_positions[i][0]-available_positions[j][0])+abs(available_positions[i][1]-available_positions[j][1])==1:\n",
    "            Union(i,j)\n",
    "      return group_size==1\n",
    "    def reset(self,grid_size=None,obstacle_len=None):\n",
    "        \"\"\"Reset the environment, ensuring Taxi, passenger, and destination are not overlapping obstacles\"\"\"\n",
    "        self.current_fuel = self.fuel_limit\n",
    "        self.passenger_picked_up = False\n",
    "        if grid_size == None:\n",
    "            grid_size = np.random.randint(5,11)\n",
    "        self.grid_size = grid_size\n",
    "        self.stations = []\n",
    "        if obstacle_len == None:\n",
    "            obstacle_len = np.random.randint(0,grid_size*grid_size)\n",
    "        while len(self.stations)<4:\n",
    "            posx,posy = np.random.randint(0,self.grid_size),np.random.randint(0,self.grid_size)\n",
    "            # check posx,posy is not adjacent to existed stations\n",
    "            if not any([abs(posx-station[0])+abs(posy-station[1])<=1 for station in self.stations]):\n",
    "                self.stations.append((posx,posy))\n",
    "        #self.stations = [(0, 0), (0, self.grid_size - 1), (self.grid_size - 1, 0), (self.grid_size - 1, self.grid_size - 1)]\n",
    "        self.obstacles = set()\n",
    "        cnt = 0\n",
    "        while len(self.obstacles)<obstacle_len and cnt < self.grid_size*self.grid_size*10:\n",
    "            posx,posy = np.random.randint(0,self.grid_size),np.random.randint(0,self.grid_size)\n",
    "            if (posx,posy) not in self.stations and (posx,posy) not in self.obstacles:\n",
    "                self.obstacles.add((posx,posy))\n",
    "            if not self.legal():\n",
    "                self.obstacles.remove((posx,posy))\n",
    "            cnt = cnt +1\n",
    "        available_positions = [\n",
    "            (x, y) for x in range(self.grid_size) for y in range(self.grid_size)\n",
    "            if (x, y) not in self.stations and (x, y) not in self.obstacles\n",
    "        ]\n",
    "\n",
    "        self.taxi_pos = random.choice(available_positions)\n",
    "\n",
    "        self.passenger_loc = random.choice([pos for pos in self.stations])\n",
    "\n",
    "\n",
    "        possible_destinations = [s for s in self.stations if s != self.passenger_loc]\n",
    "        self.destination = random.choice(possible_destinations)\n",
    "        #print(f'grid size : {self.grid_size}, obstacles : {len(self.obstacles)}, ')\n",
    "        #self.render_env(self.taxi_pos)\n",
    "        return self.get_state(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Perform an action and update the environment state.\"\"\"\n",
    "        taxi_row, taxi_col = self.taxi_pos\n",
    "        next_row, next_col = taxi_row, taxi_col\n",
    "        reward = 0\n",
    "        if action == 0 :  # Move Down\n",
    "            next_row += 1\n",
    "        elif action == 1:  # Move Up\n",
    "            next_row -= 1\n",
    "        elif action == 2:  # Move Right\n",
    "            next_col += 1\n",
    "        elif action == 3:  # Move Left\n",
    "            next_col -= 1\n",
    "\n",
    "\n",
    "        if action in [0, 1, 2, 3]:  # Only movement actions should be checked\n",
    "            if (next_row, next_col) in self.obstacles or not (0 <= next_row < self.grid_size and 0 <= next_col < self.grid_size):\n",
    "                reward -=10\n",
    "                #print('block')\n",
    "            else:\n",
    "                self.taxi_pos = (next_row, next_col)\n",
    "                if self.passenger_picked_up:\n",
    "                    self.passenger_loc = self.taxi_pos\n",
    "        else:\n",
    "            if action == 4:  # PICKUP\n",
    "                if self.taxi_pos == self.passenger_loc:\n",
    "                    self.passenger_picked_up = True\n",
    "                    self.passenger_loc = self.taxi_pos\n",
    "                else:\n",
    "                    reward = -10\n",
    "            elif action == 5:  # DROPOFF\n",
    "                if self.passenger_picked_up:\n",
    "                    self.passenger_picked_up = False\n",
    "                    self.passenger_loc = self.taxi_pos\n",
    "                    if self.taxi_pos == self.destination:\n",
    "                        reward += 50\n",
    "                        return self.get_state(), reward -0.1, True, {}\n",
    "                    else:\n",
    "                        reward -=10\n",
    "                else:\n",
    "                    reward -=10\n",
    "\n",
    "        reward -= 0.1\n",
    "\n",
    "        self.current_fuel -= 1\n",
    "        if self.current_fuel <= 0:\n",
    "            return self.get_state(), reward -10, True, {}\n",
    "\n",
    "\n",
    "\n",
    "        return self.get_state(), reward, False, {}\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Return the current environment state.\"\"\"\n",
    "        taxi_row, taxi_col = self.taxi_pos\n",
    "        passenger_row, passenger_col = self.passenger_loc\n",
    "        destination_row, destination_col = self.destination\n",
    "\n",
    "        obstacle_north = int(taxi_row == 0 or (taxi_row-1, taxi_col) in self.obstacles)\n",
    "        obstacle_south = int(taxi_row == self.grid_size - 1 or (taxi_row+1, taxi_col) in self.obstacles)\n",
    "        obstacle_east  = int(taxi_col == self.grid_size - 1 or (taxi_row, taxi_col+1) in self.obstacles)\n",
    "        obstacle_west  = int(taxi_col == 0 or (taxi_row , taxi_col-1) in self.obstacles)\n",
    "\n",
    "        passenger_loc_north = int((taxi_row - 1, taxi_col) == self.passenger_loc)\n",
    "        passenger_loc_south = int((taxi_row + 1, taxi_col) == self.passenger_loc)\n",
    "        passenger_loc_east  = int((taxi_row, taxi_col + 1) == self.passenger_loc)\n",
    "        passenger_loc_west  = int((taxi_row, taxi_col - 1) == self.passenger_loc)\n",
    "        passenger_loc_middle  = int( (taxi_row, taxi_col) == self.passenger_loc)\n",
    "        passenger_look = passenger_loc_north or passenger_loc_south or passenger_loc_east or passenger_loc_west or passenger_loc_middle\n",
    "\n",
    "        destination_loc_north = int( (taxi_row - 1, taxi_col) == self.destination)\n",
    "        destination_loc_south = int( (taxi_row + 1, taxi_col) == self.destination)\n",
    "        destination_loc_east  = int( (taxi_row, taxi_col + 1) == self.destination)\n",
    "        destination_loc_west  = int( (taxi_row, taxi_col - 1) == self.destination)\n",
    "        destination_loc_middle  = int( (taxi_row, taxi_col) == self.destination)\n",
    "        destination_look = destination_loc_north or destination_loc_south or destination_loc_east or destination_loc_west or destination_loc_middle\n",
    "\n",
    "\n",
    "        state = (taxi_row, taxi_col, self.stations[0][0],self.stations[0][1] ,self.stations[1][0],self.stations[1][1],self.stations[2][0],self.stations[2][1],self.stations[3][0],self.stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look)\n",
    "        return state,self.passenger_picked_up,self.passenger_loc\n",
    "    def render_env(self, taxi_pos,   action=None, step=None, fuel=None):\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        grid = [['.'] * self.grid_size for _ in range(self.grid_size)]\n",
    "\n",
    "        '''\n",
    "        # Place passenger\n",
    "        py, px = passenger_pos\n",
    "        if 0 <= px < self.grid_size and 0 <= py < self.grid_size:\n",
    "            grid[py][px] = 'P'\n",
    "        '''\n",
    "        stations_char =['R','G','B','Y']\n",
    "        for i in range(len(self.stations)):\n",
    "            sx,sy = self.stations[i]\n",
    "            grid[sy][sx] = stations_char[i]\n",
    "        for ox,oy in self.obstacles:\n",
    "            grid[oy][ox] = '#'\n",
    "\n",
    "        '''\n",
    "        # Place destination\n",
    "        dy, dx = destination_pos\n",
    "        if 0 <= dx < self.grid_size and 0 <= dy < self.grid_size:\n",
    "            grid[dy][dx] = 'D'\n",
    "        '''\n",
    "        # Place taxi\n",
    "        ty, tx = taxi_pos\n",
    "        if 0 <= tx < self.grid_size and 0 <= ty < self.grid_size:\n",
    "            grid[ty][tx] = '🚖'\n",
    "\n",
    "        # Print step info\n",
    "        print(f\"\\nStep: {step}\")\n",
    "        print(f\"Taxi Position: ({tx}, {ty})\")\n",
    "        #print(f\"Passenger Position: ({px}, {py}) {'(In Taxi)' if (px, py) == (tx, ty) else ''}\")\n",
    "        #print(f\"Destination: ({dx}, {dy})\")\n",
    "        print(f\"Fuel Left: {fuel}\")\n",
    "        print(f\"Last Action: {self.get_action_name(action)}\\n\")\n",
    "\n",
    "        # Print grid\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def get_action_name(self, action):\n",
    "        \"\"\"Returns a human-readable action name.\"\"\"\n",
    "        actions = [\"Move South\", \"Move North\", \"Move East\", \"Move West\", \"Pick Up\", \"Drop Off\"]\n",
    "        return actions[action] if action is not None else \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "henzhTux2qUO"
   },
   "source": [
    "## 狀態定義\n",
    "\n",
    "## 狀態定義\n",
    "\n",
    "- pos (相對 R/G/Y/B)的距離 (用 +-0)去代表，直到走到一位置確認是否 pickup/destination為止後 random 換位置。\n",
    "    - 只要相對位移>0一律用 +1表示, <0 用 -1\n",
    "- obstacles\n",
    "- passenger state\n",
    "    - 0 ⇒ not on car\n",
    "    - 1 ⇒ on car\n",
    "- candidates_p\n",
    "    - original : stations\n",
    "    - whenever one is not worked ⇒ removed it\n",
    "- mask_p= [1] * len(candidates_p) + [0] * (4-len(candidates_p))\n",
    "    - 0 代表 passenger_i 是不可能的位置\n",
    "    - 1 代表可能\n",
    "- candidates_goal\n",
    "    - original : stations\n",
    "    - whenever one is not goal ⇒ removed it\n",
    "- mask_goal= [1] * len(candidates_goal) + [0] * (4-len(candidates_goal))\n",
    "    - 0 代表 passenger_i 是不可能的位置\n",
    "    - 1 代表可能\n",
    "- pickup\n",
    "- goal_id : {0,1,2,3}\n",
    "    - 目標是 stations[goal_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKN9Ot4BpJZM"
   },
   "source": [
    "### 轉移\n",
    "\n",
    "- relative_pos = (0,0)\n",
    "    - if passenger_look\n",
    "        - candidate_p = [pos]\n",
    "        - success pickup\n",
    "            - success pickup ⇒ pickup_id=True\n",
    "            - candidates_p = [pos]\n",
    "            - goal = random choice (candidates_goal)\n",
    "    - else:\n",
    "        - candidate_p remove pos\n",
    "        - goal = random.choice(candidates_p)\n",
    "    - if goal_look\n",
    "        - candidate_g = [pos]\n",
    "    - else:\n",
    "        - candidate_g.remove(pos)\n",
    "        - goal = random choice (candidates_goal)\n",
    "- drop\n",
    "    - if pickup ⇒ candidates_p = [pos], goal_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vDSsEzx-2ndf"
   },
   "outputs": [],
   "source": [
    "from re import M\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "def tabular_q_learning(env,episodes=100000, alpha=0.1, gamma=0.99,\n",
    "                       epsilon_start=1.0, epsilon_end=0.1, decay_rate=0.99992):\n",
    "\n",
    "    global stations, candidates_p,candidates_goal, pickup\n",
    "    stations = [[0,0] for _ in range(4)]\n",
    "    candidates_p = [i for i in stations]\n",
    "    candidates_goal = [i for i in stations]\n",
    "    pickup=False\n",
    "    action_size = 6\n",
    "    q_table = defaultdict(lambda :np.zeros(action_size))\n",
    "    pickup_id = 4\n",
    "    drop_id = 5\n",
    "    ifpickup=False\n",
    "    p_loc = (0,0)\n",
    "    def cmp(a,b):\n",
    "        if a==b:\n",
    "            return 0\n",
    "        return 1 if a<b else -1\n",
    "    def get_state_obs(obs,action):\n",
    "        global stations,pickup,candidates_p,candidates_goal\n",
    "        #print(candidates_p)\n",
    "        taxi_row, taxi_col, stations[0][0], stations[0][1] , stations[1][0], stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "        agent_pos = (taxi_row,taxi_col)\n",
    "        if action==None:\n",
    "            # initialize\n",
    "            candidates_goal = [tuple(i) for i in stations]\n",
    "            candidates_p = [tuple(i) for i in stations]\n",
    "            pickup=False\n",
    "        if passenger_look:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        else:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        if destination_look:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        else:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        if action==pickup_id and not pickup and agent_pos in candidates_p:\n",
    "            pickup = True\n",
    "            candidates_p = []\n",
    "        elif action == drop_id and pickup:\n",
    "            pickup=False\n",
    "            candidates_p.append(agent_pos)\n",
    "        cmp_pos = (0,0)\n",
    "        if not pickup:\n",
    "            cmp_pos = candidates_p[0]\n",
    "        else:\n",
    "            cmp_pos = candidates_goal[0]\n",
    "        passenger_look = passenger_look and agent_pos in candidates_p\n",
    "        destination_look = destination_look and agent_pos in candidates_goal\n",
    "        relative_pos = (cmp(agent_pos[0],cmp_pos[0]),cmp(agent_pos[1],cmp_pos[1]))\n",
    "        return (relative_pos,pickup, passenger_look, destination_look, (obstacle_north,obstacle_south,obstacle_east,obstacle_west),action)\n",
    "    \"\"\"\n",
    "    if action == 0 :  # Move Down\n",
    "        next_row += 1\n",
    "    elif action == 1:  # Move Up\n",
    "        next_row -= 1\n",
    "    elif action == 2:  # Move Right\n",
    "        next_col += 1\n",
    "    elif action == 3:  # Move Left\n",
    "        next_col -= 1\n",
    "    \"\"\"\n",
    "    station_size = 4\n",
    "    total_reward = 0\n",
    "    total_reward_shaped = 0\n",
    "    cnt = [0,0,0,0]\n",
    "    epsilon = epsilon_start\n",
    "    averaged = [0,0]\n",
    "    batch_size = 2000\n",
    "    for epoch in tqdm(range(episodes+100)):\n",
    "        if epoch >=episodes:\n",
    "            epsilon = 0\n",
    "        grid_size = 5 #np.random.randint(5,11)\n",
    "        obstacle_size = np.random.randint(grid_size*grid_size)\n",
    "        obs,_ = env.reset(grid_size,obstacle_size)\n",
    "        averaged[0]+=env.grid_size\n",
    "        averaged[1]+=len(env.obstacles)\n",
    "        obs,ifpickup,p_loc = obs\n",
    "        done = False\n",
    "        state = get_state_obs(obs,action=None)\n",
    "        steps=0\n",
    "        action_l=[]\n",
    "        success = False\n",
    "        has_pickup=False\n",
    "\n",
    "        while not done:\n",
    "            \"\"\"if state[-1]!=(0,0,0,0):\n",
    "              print(state[-1])\"\"\"\n",
    "            if np.random.choice(2,p=[epsilon,1-epsilon])==0 or state not in q_table.keys():\n",
    "                action = np.random.randint(6)\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "            lst_pickup = pickup\n",
    "            relative_pos,pickup, _,_ , _,_ = state\n",
    "            obs,reward,done,_ = env.step(action)\n",
    "            obs,ifpickup,p_loc = obs\n",
    "            next_state = get_state_obs(obs,action)\n",
    "            total_reward += reward\n",
    "\n",
    "            ### reward shaping\n",
    "            reward_shaping = 0\n",
    "            \"\"\"if relative_pos == (0,0):\n",
    "                # want it to go to possible goal\n",
    "                #reward_shaping += 0.5\"\"\"\n",
    "            if relative_pos != (0,0) and action in [pickup_id,drop_id]:\n",
    "                reward_shaping -= 10\n",
    "            if done and reward>0:\n",
    "                cnt[2]+=1\n",
    "            \n",
    "            relative_pos,pickup, _,_ , _,_ = next_state\n",
    "            if not done and reward <-10 and action in [0,1,2,3]:\n",
    "                #print('hit wall')\n",
    "                cnt[-1]+=1\n",
    "            \"\"\"if not pickup:\n",
    "                reward_shaping -= p_len\n",
    "            reward_shaping -= 0.1 * goal_len\"\"\"\n",
    "            reward += reward_shaping\n",
    "            total_reward_shaped += reward\n",
    "            if epsilon:\n",
    "                q_table[state][action] = q_table[state][action] + alpha*(reward+gamma*np.max(q_table[next_state])-q_table[state][action])\n",
    "            state = next_state\n",
    "            if lst_pickup==False and pickup:\n",
    "                cnt[0]+=1\n",
    "            elif lst_pickup==True and pickup==False:\n",
    "                cnt[1]+=1\n",
    "            if pickup!=ifpickup:\n",
    "                print(pickup,ifpickup)\n",
    "            assert(pickup==ifpickup)\n",
    "\n",
    "\n",
    "        if (epoch+1)%batch_size==0:\n",
    "            cnt = [i/batch_size for i in cnt]\n",
    "            print(f'Epsilon : {epsilon}, average reward : {total_reward/batch_size:.4f}, averaged shaped reward : {total_reward_shaped/batch_size:.4f} Pickup, Drop, Success, Hit wall rate : {cnt}')\n",
    "            print(f'averaged grid size : {averaged[0]/batch_size:.2f}, averaged obstacles : {averaged[1]/batch_size:.2f}')\n",
    "            averaged = [0,0]\n",
    "            cnt = [0,0,0,0]\n",
    "            total_reward = 0\n",
    "            total_reward_shaped = 0\n",
    "        epsilon *= decay_rate #max(epsilon*decay_rate ,epsilon_end)\n",
    "    return q_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oxVEy1uSuNb",
    "outputId": "137a8cf5-0a6b-4e72-9b7c-f5a5c8dddc9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2007/100100 [01:31<37:28, 43.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.8522065114935058, average reward : -5654.4742, averaged shaped reward : -8512.1292 Pickup, Drop, Success, Hit wall rate : [18.297, 18.2935, 0.9625, 277.3275]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4019/100100 [02:01<14:09, 113.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.7261978377568737, average reward : -960.0860, averaged shaped reward : -1462.5560 Pickup, Drop, Success, Hit wall rate : [9.093, 9.0925, 0.999, 52.825]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6020/100100 [02:19<11:04, 141.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.6188210163268343, average reward : -571.5952, averaged shaped reward : -886.4102 Pickup, Drop, Success, Hit wall rate : [7.221, 7.22, 0.9985, 33.536]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8028/100100 [02:35<10:25, 147.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.5273211104987937, average reward : -353.6059, averaged shaped reward : -562.7959 Pickup, Drop, Success, Hit wall rate : [5.1265, 5.1265, 0.9995, 21.4325]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10026/100100 [02:52<09:41, 154.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.44935053309633677, average reward : -287.1861, averaged shaped reward : -466.1361 Pickup, Drop, Success, Hit wall rate : [4.7805, 4.7805, 1.0, 17.7155]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12017/100100 [03:06<11:00, 133.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.38290881509175456, average reward : -164.2493, averaged shaped reward : -280.2843 Pickup, Drop, Success, Hit wall rate : [3.7125, 3.7125, 1.0, 11.297]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14026/100100 [03:20<11:25, 125.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.32629128013861114, average reward : -130.2755, averaged shaped reward : -224.4904 Pickup, Drop, Success, Hit wall rate : [2.523, 2.523, 0.999, 9.0295]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16028/100100 [03:35<09:40, 144.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.27804530817339046, average reward : -116.9920, averaged shaped reward : -206.1370 Pickup, Drop, Success, Hit wall rate : [3.067, 3.067, 1.0, 8.6935]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18004/100100 [03:50<16:51, 81.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.23693306595381175, average reward : -91.4889, averaged shaped reward : -167.7539 Pickup, Drop, Success, Hit wall rate : [2.742, 2.742, 1.0, 7.11]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 20016/100100 [04:05<09:31, 140.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.20189974832183086, average reward : -78.8779, averaged shaped reward : -147.4679 Pickup, Drop, Success, Hit wall rate : [2.4585, 2.4585, 0.9995, 6.3765]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22037/100100 [04:21<07:43, 168.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.17204651536634863, average reward : -78.0082, averaged shaped reward : -148.0532 Pickup, Drop, Success, Hit wall rate : [2.761, 2.761, 0.9995, 6.336]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24015/100100 [04:34<07:24, 171.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.14660743114211577, average reward : -18.1903, averaged shaped reward : -53.7204 Pickup, Drop, Success, Hit wall rate : [1.603, 1.603, 1.0, 3.1315]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26027/100100 [04:48<07:36, 162.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.1249298122680501, average reward : -25.3068, averaged shaped reward : -63.5168 Pickup, Drop, Success, Hit wall rate : [1.7985, 1.7985, 1.0, 3.5725]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28014/100100 [05:00<08:07, 147.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.106457482214534, average reward : -7.1651, averaged shaped reward : -33.7301 Pickup, Drop, Success, Hit wall rate : [1.2775, 1.2775, 0.999, 2.5675]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 30027/100100 [05:13<07:35, 154.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.09071650163966652, average reward : 3.8540, averaged shaped reward : -18.1260 Pickup, Drop, Success, Hit wall rate : [1.2785, 1.2785, 1.0, 1.971]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32029/100100 [05:26<08:45, 129.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.07730300866176328, average reward : 17.1972, averaged shaped reward : 2.0672 Pickup, Drop, Success, Hit wall rate : [1.2505, 1.2505, 1.0, 1.429]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34011/100100 [05:40<07:43, 142.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.06587285708940639, average reward : -1.7307, averaged shaped reward : -24.9707 Pickup, Drop, Success, Hit wall rate : [1.4165, 1.4165, 1.0, 2.2095]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36033/100100 [05:57<08:31, 125.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.05613278676005368, average reward : -10.7890, averaged shaped reward : -40.0590 Pickup, Drop, Success, Hit wall rate : [1.7845, 1.7845, 0.9995, 2.53]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38031/100100 [06:11<06:19, 163.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.04783289944708348, average reward : 11.3802, averaged shaped reward : -6.0698 Pickup, Drop, Success, Hit wall rate : [1.441, 1.4405, 0.9995, 1.57]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 40036/100100 [06:27<09:52, 101.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.040760247291748986, average reward : 7.4038, averaged shaped reward : -15.3512 Pickup, Drop, Success, Hit wall rate : [1.66, 1.6585, 0.9985, 2.09]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42008/100100 [06:44<09:27, 102.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.03473336926026185, average reward : -15.7713, averaged shaped reward : -43.5114 Pickup, Drop, Success, Hit wall rate : [1.6145, 1.6135, 0.9975, 3.337]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44011/100100 [07:02<06:51, 136.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.02959763544942752, average reward : -9.5519, averaged shaped reward : -29.0969 Pickup, Drop, Success, Hit wall rate : [1.3485, 1.348, 0.997, 2.8715]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46020/100100 [07:17<06:34, 137.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.02522127979100079, average reward : 20.7925, averaged shaped reward : 11.0675 Pickup, Drop, Success, Hit wall rate : [1.16, 1.16, 0.9995, 1.1495]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48033/100100 [07:29<06:11, 140.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.021492019366981217, average reward : 35.7249, averaged shaped reward : 31.3049 Pickup, Drop, Success, Hit wall rate : [1.064, 1.064, 1.0, 0.476]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 50039/100100 [07:42<04:10, 199.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.018314173598578017, average reward : 35.8130, averaged shaped reward : 31.3530 Pickup, Drop, Success, Hit wall rate : [1.098, 1.098, 1.0, 0.389]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52029/100100 [07:55<04:48, 166.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.015606209396691164, average reward : 33.2892, averaged shaped reward : 28.5892 Pickup, Drop, Success, Hit wall rate : [1.1, 1.0995, 0.999, 0.461]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54020/100100 [08:08<04:55, 155.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.01329864929052995, average reward : 36.6805, averaged shaped reward : 33.0906 Pickup, Drop, Success, Hit wall rate : [1.078, 1.078, 1.0, 0.332]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56030/100100 [08:22<04:30, 163.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.011332288863816558, average reward : 35.8154, averaged shaped reward : 32.2754 Pickup, Drop, Success, Hit wall rate : [1.0655, 1.065, 0.9995, 0.3125]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58036/100100 [08:34<04:42, 148.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.009656677763841059, average reward : 38.6096, averaged shaped reward : 36.1746 Pickup, Drop, Success, Hit wall rate : [1.035, 1.035, 1.0, 0.2325]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 60024/100100 [08:53<05:08, 129.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.008228825311046321, average reward : 18.6501, averaged shaped reward : 13.2601 Pickup, Drop, Success, Hit wall rate : [1.0545, 1.0535, 0.995, 0.828]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62011/100100 [09:11<08:15, 76.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.007012097499335272, average reward : 11.9273, averaged shaped reward : 7.1173 Pickup, Drop, Success, Hit wall rate : [1.042, 1.0415, 0.995, 1.523]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64011/100100 [09:34<07:05, 84.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0059752770877490105, average reward : -8.3240, averaged shaped reward : -16.2790 Pickup, Drop, Success, Hit wall rate : [1.022, 1.022, 0.99, 2.3875]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66019/100100 [10:01<05:09, 110.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.005091762668554272, average reward : -41.9859, averaged shaped reward : -60.0909 Pickup, Drop, Success, Hit wall rate : [1.0025, 1.002, 0.9775, 3.6595]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68005/100100 [10:29<03:26, 155.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.004338886162457397, average reward : -6.0330, averaged shaped reward : -16.9280 Pickup, Drop, Success, Hit wall rate : [1.013, 1.013, 0.9835, 1.5755]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 70011/100100 [10:51<04:07, 121.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0036973312301120526, average reward : 21.8261, averaged shaped reward : 18.2761 Pickup, Drop, Success, Hit wall rate : [1.016, 1.016, 0.989, 0.4165]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72016/100100 [11:09<03:25, 136.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0031506376782698244, average reward : 31.4833, averaged shaped reward : 29.2833 Pickup, Drop, Success, Hit wall rate : [1.02, 1.02, 0.9975, 0.239]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74001/100100 [11:36<07:35, 57.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.002684779145262747, average reward : -6.3854, averaged shaped reward : -27.0804 Pickup, Drop, Success, Hit wall rate : [1.0835, 1.074, 0.977, 1.8845]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76008/100100 [12:09<08:09, 49.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.002287803230613323, average reward : -30.9403, averaged shaped reward : -58.7453 Pickup, Drop, Success, Hit wall rate : [1.404, 1.3965, 0.981, 3.2535]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78016/100100 [12:39<04:52, 75.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0019495248356797433, average reward : -26.7095, averaged shaped reward : -49.3996 Pickup, Drop, Success, Hit wall rate : [1.4955, 1.486, 0.9825, 2.5675]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 80007/100100 [13:15<05:58, 56.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.00166126484746384, average reward : -46.0794, averaged shaped reward : -68.1194 Pickup, Drop, Success, Hit wall rate : [1.2615, 1.249, 0.9635, 2.687]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82008/100100 [13:56<05:05, 59.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0014156274610663245, average reward : -63.8373, averaged shaped reward : -89.6473 Pickup, Drop, Success, Hit wall rate : [1.2205, 1.2065, 0.946, 3.0]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84004/100100 [14:34<07:09, 37.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.001206310427614528, average reward : -47.1406, averaged shaped reward : -67.9806 Pickup, Drop, Success, Hit wall rate : [1.317, 1.294, 0.947, 2.512]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86004/100100 [15:01<03:07, 75.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0010279433592475116, average reward : 1.8630, averaged shaped reward : -3.9569 Pickup, Drop, Success, Hit wall rate : [1.0015, 0.9935, 0.965, 0.543]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88005/100100 [15:33<05:58, 33.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.000875949942595302, average reward : -2.8307, averaged shaped reward : -7.9057 Pickup, Drop, Success, Hit wall rate : [1.001, 0.9815, 0.9515, 0.3195]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 90017/100100 [16:07<02:45, 61.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.000746430525602493, average reward : -9.1116, averaged shaped reward : -15.4915 Pickup, Drop, Success, Hit wall rate : [1.023, 1.009, 0.954, 0.4705]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92004/100100 [16:38<01:50, 72.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0006360620652596216, average reward : -3.0020, averaged shaped reward : -7.8169 Pickup, Drop, Success, Hit wall rate : [0.995, 0.9845, 0.96, 0.3805]\n",
      "averaged grid size : 5.00, averaged obstacles : 9.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94009/100100 [17:06<01:32, 65.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0005420128692295586, average reward : 7.3761, averaged shaped reward : 5.2210 Pickup, Drop, Success, Hit wall rate : [0.987, 0.9765, 0.9645, 0.171]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96007/100100 [17:33<00:37, 108.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.00046186994391898806, average reward : 8.2460, averaged shaped reward : 6.5510 Pickup, Drop, Success, Hit wall rate : [0.9885, 0.9775, 0.964, 0.136]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98017/100100 [18:01<00:20, 104.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0003935770849850095, average reward : 8.8495, averaged shaped reward : 7.0645 Pickup, Drop, Success, Hit wall rate : [0.9875, 0.9735, 0.9615, 0.102]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 100002/100100 [18:29<00:01, 51.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon : 0.0003353821218824913, average reward : 9.0388, averaged shaped reward : 4.5237 Pickup, Drop, Success, Hit wall rate : [1.0255, 1.0085, 0.964, 0.2075]\n",
      "averaged grid size : 5.00, averaged obstacles : 10.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100100/100100 [18:31<00:00, 90.04it/s]\n"
     ]
    }
   ],
   "source": [
    "env = SimpleTaxiEnv()\n",
    "q_table = tabular_q_learning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "FaVsGr6fbI7l"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "q_table_dict = dict(q_table)  # Convert to regular dict\n",
    "with open('q_table.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "1yAJ9k3vWNPX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1909\n"
     ]
    }
   ],
   "source": [
    "print(len(q_table_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXo_X3thVos1"
   },
   "source": [
    "# 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def Testing(env):\n",
    "    import pickle\n",
    "    from collections import defaultdict\n",
    "    with open('q_table.pkl', 'rb') as f:\n",
    "        print('load')\n",
    "        loaded_dict = pickle.load(f)\n",
    "    q_table = defaultdict(lambda: np.zeros(6), loaded_dict)  # Replace 0 with your default value\n",
    "    print(len(q_table))\n",
    "    #print('len of q_table',len(q_table.keys()))\n",
    "    global stations, candidates_p,candidates_goal, pickup, goal_id,last_action\n",
    "    stations = [[0,0] for _ in range(4)]\n",
    "    candidates_p = [i for i in stations]\n",
    "    candidates_goal = [i for i in stations]\n",
    "    goal_id = -1\n",
    "    pickup=False\n",
    "    action_size = 6\n",
    "    last_action = None\n",
    "    #q_table = defaultdict(lambda :np.zeros(action_size))\n",
    "    pickup_id = 4\n",
    "    drop_id = 5\n",
    "    ifpickup=False\n",
    "    p_loc = (0,0)\n",
    "    def cmp(a,b):\n",
    "        if a==b:\n",
    "            return 0\n",
    "        return 1 if a<b else -1\n",
    "            \n",
    "    def get_state_obs(obs,action):\n",
    "        global stations,pickup,candidates_p,candidates_goal\n",
    "        #print(candidates_p)\n",
    "        taxi_row, taxi_col, stations[0][0], stations[0][1] , stations[1][0], stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "        agent_pos = (taxi_row,taxi_col)\n",
    "        if action==None:\n",
    "            # initialize\n",
    "            candidates_goal = [tuple(i) for i in stations]\n",
    "            candidates_p = [tuple(i) for i in stations]\n",
    "            pickup=False\n",
    "        if passenger_look:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        else:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        if destination_look:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        else:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        if action==pickup_id and not pickup and agent_pos in candidates_p:\n",
    "            pickup = True\n",
    "            candidates_p = []\n",
    "        elif action == drop_id and pickup:\n",
    "            pickup=False\n",
    "            candidates_p.append(agent_pos)\n",
    "        cmp_pos = (0,0)\n",
    "        if not pickup:\n",
    "            cmp_pos = candidates_p[0]\n",
    "        else:\n",
    "            cmp_pos = candidates_goal[0]\n",
    "        passenger_look = passenger_look and agent_pos in candidates_p\n",
    "        destination_look = destination_look and agent_pos in candidates_goal\n",
    "        relative_pos = (cmp(agent_pos[0],cmp_pos[0]),cmp(agent_pos[1],cmp_pos[1]))\n",
    "        return (relative_pos,pickup, passenger_look, destination_look, (obstacle_north,obstacle_south,obstacle_east,obstacle_west),action)\n",
    "\n",
    "    def get_action(obs):\n",
    "        # TODO: Train your own agent\n",
    "        # HINT: If you're using a Q-table, consider designing a custom key based on `obs` to store useful information.\n",
    "        # NOTE: Keep in mind that your Q-table may not cover all possible states in the testing environment.\n",
    "        #       To prevent crashes, implement a fallback strategy for missing keys.\n",
    "        #       Otherwise, even if your agent performs well in training, it may fail during testing.\n",
    "        global last_action\n",
    "        state = get_state_obs(obs,last_action)\n",
    "        action_name = ['Move North','Move South','Move East','Move West','Pick Up','Drop Off']\n",
    "        if state not in q_table.keys():\n",
    "            #print(state)\n",
    "            print(state)\n",
    "            assert(0)\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            #print(state,action_name[np.argmax(q_table[state])])\n",
    "            action = np.argmax(q_table[state])\n",
    "        #q_table[state][action] = q_table[state][action] + 0.089487*(-0.1+0.89487*np.max(q_table[state])-q_table[state][action])\n",
    "        last_action=action\n",
    "        return action # Choose a random action\n",
    "        # You can submit this random agent to evaluate the performance of a purely random strategy.\n",
    "    Total_reward=0\n",
    "    for i in tqdm(range(100)):\n",
    "        grid_size = np.random.randint(5,11)\n",
    "        obstacle_size = np.random.randint(grid_size*grid_size)\n",
    "        obs,_ = env.reset()\n",
    "        obs,_,_ = obs\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        stations = [[0,0] for _ in range(4)]\n",
    "        candidates_p = [i for i in stations]\n",
    "        candidates_goal = [i for i in stations]\n",
    "        goal_id = -1\n",
    "        pickup=False\n",
    "        action_size = 6\n",
    "        pickup_id = 4\n",
    "        drop_id = 5\n",
    "        last_action = None\n",
    "        lst_a = None\n",
    "        while not done:\n",
    "            assert(last_action==lst_a)\n",
    "            action = get_action(obs)\n",
    "            obs,reward,done,_ = env.step(action)\n",
    "            obs,_,_ = obs\n",
    "            total_reward += reward\n",
    "            lst_a=action\n",
    "        Total_reward+=total_reward\n",
    "        print(f'grid_size : {env.grid_size}, obstacle_size : {len(env.obstacles)}, total_reward : {total_reward}')\n",
    "    print(f'average : {Total_reward/100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "1909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:00<00:02, 33.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 5, obstacle_size : 12, total_reward : 47.0\n",
      "grid_size : 9, obstacle_size : 14, total_reward : 24.599999999999994\n",
      "grid_size : 6, obstacle_size : 11, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 35, total_reward : 47.4\n",
      "grid_size : 9, obstacle_size : 69, total_reward : 48.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:00<00:03, 23.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 8, obstacle_size : 46, total_reward : 48.3\n",
      "grid_size : 6, obstacle_size : 3, total_reward : 49.1\n",
      "grid_size : 9, obstacle_size : 44, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 16, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 1, total_reward : 48.1\n",
      "grid_size : 9, obstacle_size : 46, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:00<00:03, 25.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 7, obstacle_size : 34, total_reward : -33739.99999999689\n",
      "grid_size : 6, obstacle_size : 23, total_reward : 49.1\n",
      "grid_size : 5, obstacle_size : 9, total_reward : 47.5\n",
      "grid_size : 10, obstacle_size : 34, total_reward : -510.0000000000452\n",
      "grid_size : 9, obstacle_size : 40, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 13, total_reward : 47.8\n",
      "grid_size : 8, obstacle_size : 6, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 34, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:00<00:02, 30.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 8, obstacle_size : 47, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 15, total_reward : 48.8\n",
      "grid_size : 7, obstacle_size : 0, total_reward : 47.099999999999994\n",
      "grid_size : 6, obstacle_size : 25, total_reward : 47.0\n",
      "grid_size : 5, obstacle_size : 5, total_reward : 37.9\n",
      "grid_size : 10, obstacle_size : 1, total_reward : 46.5\n",
      "grid_size : 8, obstacle_size : 48, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 36, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 19, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:01<00:02, 27.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 34, total_reward : -510.0000000000452\n",
      "grid_size : 5, obstacle_size : 15, total_reward : 49.4\n",
      "grid_size : 8, obstacle_size : 46, total_reward : 48.6\n",
      "grid_size : 10, obstacle_size : 20, total_reward : 48.1\n",
      "grid_size : 9, obstacle_size : 46, total_reward : 46.9\n",
      "grid_size : 9, obstacle_size : 22, total_reward : -510.0000000000452\n",
      "grid_size : 9, obstacle_size : 3, total_reward : 48.199999999999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:01<00:02, 23.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 9, obstacle_size : 57, total_reward : 43.7\n",
      "grid_size : 7, obstacle_size : 5, total_reward : 47.699999999999996\n",
      "grid_size : 7, obstacle_size : 27, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 14, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 8, total_reward : 47.699999999999996\n",
      "grid_size : 9, obstacle_size : 29, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 21, total_reward : 48.6\n",
      "grid_size : 6, obstacle_size : 5, total_reward : 48.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:01<00:01, 28.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 6, obstacle_size : 13, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 21, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:02<00:02, 23.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 68, total_reward : 31.899999999999977\n",
      "grid_size : 6, obstacle_size : 4, total_reward : 47.5\n",
      "grid_size : 6, obstacle_size : 23, total_reward : 48.4\n",
      "grid_size : 7, obstacle_size : 25, total_reward : 47.9\n",
      "grid_size : 5, obstacle_size : 15, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 24, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 52, total_reward : 48.1\n",
      "grid_size : 5, obstacle_size : 4, total_reward : 48.0\n",
      "grid_size : 9, obstacle_size : 64, total_reward : 48.4\n",
      "grid_size : 7, obstacle_size : 6, total_reward : 49.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:02<00:01, 24.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 57, total_reward : 43.60000000000001\n",
      "grid_size : 8, obstacle_size : 7, total_reward : 45.8\n",
      "grid_size : 8, obstacle_size : 27, total_reward : -25369.99999999826\n",
      "grid_size : 6, obstacle_size : 20, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 27, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 45, total_reward : 47.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [00:02<00:01, 23.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 5, obstacle_size : 15, total_reward : -25499.99999999823\n",
      "grid_size : 9, obstacle_size : 20, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 46, total_reward : 48.9\n",
      "grid_size : 7, obstacle_size : 34, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:02<00:01, 20.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 36, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 26, total_reward : -510.0000000000452\n",
      "grid_size : 9, obstacle_size : 56, total_reward : 46.8\n",
      "grid_size : 7, obstacle_size : 7, total_reward : 48.4\n",
      "grid_size : 9, obstacle_size : 51, total_reward : 47.099999999999994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:03<00:01, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 7, obstacle_size : 5, total_reward : 47.8\n",
      "grid_size : 5, obstacle_size : 0, total_reward : 47.199999999999996\n",
      "grid_size : 9, obstacle_size : 53, total_reward : -510.0000000000452\n",
      "grid_size : 7, obstacle_size : 8, total_reward : 46.0\n",
      "grid_size : 5, obstacle_size : 0, total_reward : 49.0\n",
      "grid_size : 9, obstacle_size : 8, total_reward : 47.4\n",
      "grid_size : 6, obstacle_size : 16, total_reward : 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:03<00:00, 25.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 75, total_reward : 48.5\n",
      "grid_size : 6, obstacle_size : 17, total_reward : 48.699999999999996\n",
      "grid_size : 5, obstacle_size : 13, total_reward : 48.9\n",
      "grid_size : 5, obstacle_size : 3, total_reward : 48.4\n",
      "grid_size : 8, obstacle_size : 46, total_reward : 48.5\n",
      "grid_size : 10, obstacle_size : 72, total_reward : 47.8\n",
      "grid_size : 6, obstacle_size : 19, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:03<00:00, 22.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 10, obstacle_size : 39, total_reward : -5040.000000000077\n",
      "grid_size : 8, obstacle_size : 47, total_reward : 46.9\n",
      "grid_size : 9, obstacle_size : 18, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 23, total_reward : 49.199999999999996\n",
      "grid_size : 6, obstacle_size : 27, total_reward : -510.0000000000452\n",
      "grid_size : 6, obstacle_size : 24, total_reward : -510.0000000000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 25.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_size : 7, obstacle_size : 29, total_reward : -510.0000000000452\n",
      "grid_size : 8, obstacle_size : 3, total_reward : 48.3\n",
      "grid_size : 9, obstacle_size : 55, total_reward : 48.1\n",
      "grid_size : 9, obstacle_size : 46, total_reward : 47.4\n",
      "grid_size : 6, obstacle_size : 25, total_reward : 48.3\n",
      "grid_size : 6, obstacle_size : 18, total_reward : 47.9\n",
      "grid_size : 7, obstacle_size : 12, total_reward : -510.0000000000452\n",
      "grid_size : 10, obstacle_size : 7, total_reward : 46.599999999999994\n",
      "grid_size : 6, obstacle_size : 15, total_reward : 48.9\n",
      "grid_size : 8, obstacle_size : 15, total_reward : 46.0\n",
      "average : -1040.7869999999496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = SimpleTaxiEnv()\n",
    "Testing(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSxQEFroMcJw"
   },
   "source": [
    "# Policy base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "nP4NtkKwMi1s"
   },
   "outputs": [],
   "source": [
    "def tabular_policy_learning(env,episodes=5000, alpha=0.1, gamma=0.999):\n",
    "\n",
    "    global stations, candidates_p,candidates_goal, pickup\n",
    "    stations = [[0,0] for _ in range(4)]\n",
    "    candidates_p = [i for i in stations]\n",
    "    candidates_goal = [i for i in stations]\n",
    "    pickup=False\n",
    "    action_size = 6\n",
    "    policy_table = defaultdict(lambda :np.zeros(action_size))\n",
    "    pickup_id = 4\n",
    "    drop_id = 5\n",
    "    ifpickup=False\n",
    "    p_loc = (0,0)\n",
    "    def softmax(x):\n",
    "        \"\"\"✅ Compute softmax values for an array.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))  # Numeric stability\n",
    "        return exp_x / exp_x.sum()\n",
    "    def get_state_obs(obs,action):\n",
    "        global stations,pickup,candidates_p,candidates_goal\n",
    "        #print(candidates_p)\n",
    "        obstacles = [0 for _ in range(5)]\n",
    "        taxi_row, taxi_col, stations[0][0], stations[0][1] , stations[1][0], stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "        agent_pos = (taxi_row,taxi_col)\n",
    "        if action==None:\n",
    "            # initialize\n",
    "            candidates_goal = [tuple(i) for i in stations]\n",
    "            candidates_p = [tuple(i) for i in stations]\n",
    "            pickup=False\n",
    "        if passenger_look:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        else:\n",
    "            #print('before p',candidates_p)\n",
    "            candidates_p = [ tuple(x) for x in candidates_p if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after p',candidates_p)\n",
    "        if destination_look:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) <=1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        else:\n",
    "            #print('before g',candidates_goal)\n",
    "            candidates_goal = [ tuple(x) for x in candidates_goal if abs(x[0]-agent_pos[0])+abs(x[1]-agent_pos[1]) >1 ]\n",
    "            #print('after g',candidates_goal)\n",
    "        if action==pickup_id and not pickup and agent_pos in candidates_p:\n",
    "            pickup = True\n",
    "            candidates_p = []\n",
    "        elif action == drop_id and pickup:\n",
    "            pickup=False\n",
    "            candidates_p = [agent_pos]\n",
    "        return (pickup, len(candidates_p), len(candidates_goal), passenger_look, destination_look, (obstacle_north,obstacle_south,obstacle_east,obstacle_west))\n",
    "    \"\"\"\n",
    "    if action == 0 :  # Move Down\n",
    "        next_row += 1\n",
    "    elif action == 1:  # Move Up\n",
    "        next_row -= 1\n",
    "    elif action == 2:  # Move Right\n",
    "        next_col += 1\n",
    "    elif action == 3:  # Move Left\n",
    "        next_col -= 1\n",
    "    \"\"\"\n",
    "    station_size = 4\n",
    "    total_reward = 0\n",
    "    total_reward_shaped = 0\n",
    "    cnt = [0,0,0]\n",
    "    averaged = [0,0]\n",
    "    batch_size = 100\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(episodes+600)):\n",
    "        if epoch >= episodes:\n",
    "            epsilon = 0\n",
    "        grid_size = np.random.randint(5,11)\n",
    "        obstacle_size = np.random.randint(grid_size*grid_size)\n",
    "        obs,_ = env.reset()\n",
    "        averaged[0]+=env.grid_size\n",
    "        averaged[1]+=len(env.obstacles)\n",
    "        obs,ifpickup,p_loc = obs\n",
    "        done = False\n",
    "        state = get_state_obs(obs,action=None)\n",
    "        steps=0\n",
    "        action_l=[]\n",
    "        success = False\n",
    "        has_pickup=False\n",
    "        trajectory = []\n",
    "        while not done:\n",
    "            \"\"\"if state[-1]!=(0,0,0,0):\n",
    "              print(state[-1])\"\"\"\n",
    "            action = np.random.choice(action_size,p=softmax(policy_table[state]))\n",
    "            lst_pickup = pickup\n",
    "            obs,reward,done,_ = env.step(action)\n",
    "            obs,ifpickup,p_loc = obs\n",
    "            next_state = get_state_obs(obs,action)\n",
    "            total_reward += reward\n",
    "            if done and reward>0:\n",
    "                cnt[2]+=1\n",
    "            reward_shaping = 0\n",
    "            pickup, p_len, goal_len, _,_ , _ = next_state\n",
    "            if not pickup:\n",
    "                reward_shaping -= 0.01 * (p_len**2)\n",
    "            reward_shaping -= 0.001 * (goal_len**2)\n",
    "            reward += reward_shaping\n",
    "            total_reward_shaped += reward\n",
    "            trajectory.append((state,action,reward))\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "            if lst_pickup==False and pickup:\n",
    "                cnt[0]+=1\n",
    "            elif lst_pickup==True and pickup==False:\n",
    "                cnt[1]+=1\n",
    "            if pickup!=ifpickup:\n",
    "                print(pickup,ifpickup)\n",
    "            assert(pickup==ifpickup)\n",
    "        G = 0\n",
    "        for t in reversed(trajectory):\n",
    "            state,action,reward = t\n",
    "            G = gamma*G + reward\n",
    "            prob = softmax(policy_table[state])\n",
    "            grad=-prob.copy()\n",
    "            grad[action]+=1\n",
    "            policy_table[state] += alpha*G*grad\n",
    "        if (epoch+1)%batch_size==0:\n",
    "            cnt = [i/batch_size for i in cnt]\n",
    "            print(f'Epoch : {epoch}, average reward : {total_reward/batch_size:.4f}, averaged shaped reward : {total_reward_shaped/batch_size:.4f} Pickup, Drop, Success rate : {cnt}')\n",
    "            print(f'averaged grid size : {averaged[0]/batch_size:.2f}, averaged obstacles : {averaged[1]/batch_size:.2f}')\n",
    "            averaged = [0,0]\n",
    "            cnt = [0,0,0]\n",
    "            total_reward = 0\n",
    "            total_reward_shaped = 0\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8pe7Sg6NarrP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/5600 [00:04<20:19,  4.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m SimpleTaxiEnv()\n\u001b[1;32m----> 2\u001b[0m policy_table \u001b[38;5;241m=\u001b[39m \u001b[43mtabular_policy_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# save policy table to .pkl\u001b[39;00m\n\u001b[0;32m      4\u001b[0m policy_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(policy_table)  \u001b[38;5;66;03m# Convert to regular dict\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 120\u001b[0m, in \u001b[0;36mtabular_policy_learning\u001b[1;34m(env, episodes, alpha, gamma)\u001b[0m\n\u001b[0;32m    118\u001b[0m state,action,reward \u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m    119\u001b[0m G \u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m*\u001b[39mG \u001b[38;5;241m+\u001b[39m reward\n\u001b[1;32m--> 120\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mprob\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    122\u001b[0m grad[action]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[44], line 17\u001b[0m, in \u001b[0;36mtabular_policy_learning.<locals>.softmax\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"✅ Compute softmax values for an array.\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m exp_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(x \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x))  \u001b[38;5;66;03m# Numeric stability\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp_x \u001b[38;5;241m/\u001b[39m \u001b[43mexp_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\_core\\_methods.py:51\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = SimpleTaxiEnv()\n",
    "policy_table = tabular_policy_learning(env)\n",
    "# save policy table to .pkl\n",
    "policy_dict = dict(policy_table)  # Convert to regular dict\n",
    "with open('policy_table.pkl', 'wb') as f:\n",
    "    pickle.dump(policy_dict, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vBm02Hg7kQwD",
    "ioWeQB4DTncX"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
